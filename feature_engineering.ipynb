{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment Questions**"
      ],
      "metadata": {
        "id": "Cc4fxgBizh_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. What is a parameter?**"
      ],
      "metadata": {
        "id": "n5uSEc4Ezq2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In computer programming, a parameter (also known as a formal parameter) is a special kind of variable used in a function, method, or subroutine definition. It acts as a placeholder for the actual values that will be passed into the function when it is called.\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "- When you define a function, you specify parameters to indicate what kind of information the function needs to do its job.\n",
        "- When you call the function, you provide arguments, which are the actual values or expressions that get assigned to those parameters.\n"
      ],
      "metadata": {
        "id": "p71e8gqn0EYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. What is correlation?**\n",
        "# **What does negative correlation mean?**\n"
      ],
      "metadata": {
        "id": "EVBYikDn0Ki3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6361af36"
      },
      "source": [
        "Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). It's a common tool for describing simple relationships without making a statement about cause and effect.\n",
        "\n",
        "Key aspects of correlation:\n",
        "\n",
        "*   **Strength:** How closely the two variables move together. This is indicated by the absolute value of the correlation coefficient.\n",
        "*   **Direction:** Whether the variables tend to increase or decrease together (positive correlation) or whether one variable tends to increase as the other decreases (negative correlation).\n",
        "\n",
        "**Correlation Coefficient:**\n",
        "\n",
        "The most common measure of correlation is the Pearson correlation coefficient (r), which ranges from -1 to +1:\n",
        "\n",
        "*   **+1:** Perfect positive correlation (as one variable increases, the other increases proportionally).\n",
        "*   **-1:** Perfect negative correlation (as one variable increases, the other decreases proportionally).\n",
        "*   **0:** No linear correlation (the variables do not show a linear relationship).\n",
        "\n",
        "**Important Note:** Correlation does **not** imply causation. Just because two variables are correlated doesn't mean that one causes the other. There might be a third, unobserved variable influencing both, or the correlation could be purely coincidental."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Define Machine Learning. What are the main components in Machine Learning?**"
      ],
      "metadata": {
        "id": "OQJdLVL71E-X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7962829e"
      },
      "source": [
        "## What is Machine Learning?\n",
        "\n",
        "**Machine Learning (ML)** is a subset of Artificial Intelligence (AI) that enables systems to learn from data, identify patterns, and make decisions or predictions with minimal human intervention. Instead of being explicitly programmed for every task, ML algorithms build a mathematical model based on sample data, known as \"training data,\" to make predictions or decisions without being explicitly programmed to perform the task.\n",
        "\n",
        "Think of it as teaching a computer to learn from experience, much like humans do. The more data an ML model is exposed to, the better it becomes at recognizing patterns and making accurate predictions.\n",
        "\n",
        "## Main Components of Machine Learning:\n",
        "\n",
        "Machine Learning systems generally consist of several key components that work together:\n",
        "\n",
        "1.  **Data:** This is the most crucial component. ML models learn from data, so the quality, quantity, and relevance of the data directly impact the model's performance. Data can be numerical, categorical, textual, images, audio, etc.\n",
        "\n",
        "2.  **Features:** These are individual measurable properties or characteristics of the phenomenon being observed. In a dataset, features are the columns that describe the data points (rows). Selecting relevant features (feature engineering) is a critical step.\n",
        "\n",
        "3.  **Algorithm:** This is the set of rules or instructions that the machine uses to learn from the data. Different algorithms are suited for different types of problems and data (e.g., linear regression, decision trees, neural networks, support vector machines).\n",
        "\n",
        "4.  **Model:** The model is the output of the learning process. It's the mathematical representation or function that the algorithm has learned from the training data. This model is then used to make predictions or classifications on new, unseen data.\n",
        "\n",
        "5.  **Training:** This is the process where the ML algorithm is fed with data (training data) and learns the patterns and relationships within that data. During training, the model's parameters are adjusted to minimize errors and improve its predictive accuracy.\n",
        "\n",
        "6.  **Evaluation:** After training, the model's performance is assessed using a separate dataset called \"test data\" (or validation data). Evaluation metrics (e.g., accuracy, precision, recall, F1-score, mean squared error) are used to quantify how well the model generalizes to new data.\n",
        "\n",
        "7.  **Prediction/Inference:** Once the model is trained and evaluated, it can be used to make predictions or decisions on new, previously unseen data. This is often referred to as inference."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. How does loss value help in determining whether the model is good or not?**"
      ],
      "metadata": {
        "id": "xqQa7Q2J1Vwr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47f289e8"
      },
      "source": [
        "The **loss value** (also known as the cost function or error function) is a measure of how far off a model's predictions are from the actual true values. In simpler terms, it quantifies the \"error\" a model makes. During the training process of a machine learning model, the goal is typically to minimize this loss value.\n",
        "\n",
        "Here's how the loss value helps in determining whether a model is good or not:\n",
        "\n",
        "1.  **Quantifies Performance:** The loss value provides a numerical score for the model's performance on a given dataset. A lower loss value generally indicates a better-performing model because its predictions are closer to the actual values.\n",
        "\n",
        "2.  **Guides Optimization:** During training, the model's parameters (e.g., weights and biases in a neural network) are adjusted iteratively to reduce the loss. Optimization algorithms (like gradient descent) use the calculated loss to determine the direction and magnitude of these adjustments. A decreasing loss over training epochs suggests that the model is learning and improving.\n",
        "\n",
        "3.  **Detects Underfitting and Overfitting:**\n",
        "    *   **High Loss on Training Data (Underfitting):** If the loss value is high even on the training data, it suggests that the model is too simple or hasn't learned the underlying patterns in the data effectively. This is called underfitting, and the model won't perform well on new data either.\n",
        "    *   **Low Loss on Training Data but High Loss on Validation/Test Data (Overfitting):** If the loss on the training data is very low, but the loss on a separate validation or test dataset is significantly higher, it indicates that the model has memorized the training data rather than learning generalizable patterns. This is called overfitting, and the model will perform poorly on unseen data.\n",
        "\n",
        "4.  **Comparison Between Models:** Loss values can be used to compare the performance of different models on the same task. The model with consistently lower loss on unseen data (validation/test sets) is generally considered better.\n",
        "\n",
        "5.  **Stopping Criterion:** The loss value can serve as a stopping criterion for training. Training might stop when the loss on the validation set stops decreasing or starts to increase (to prevent overfitting).\n",
        "\n",
        "In essence, the loss value acts as a guide, providing continuous feedback during the model development process, helping to steer the model towards better predictions and indicating its overall quality."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. What are continuous and categorical variables?**"
      ],
      "metadata": {
        "id": "zbwempT61e4f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69f5070a"
      },
      "source": [
        "In statistics and data analysis, variables are generally classified into two main types based on the nature of the data they represent:\n",
        "\n",
        "### 1. Continuous Variables\n",
        "\n",
        "**Definition:** A continuous variable is a variable that can take any value within a given range. This means there are an infinite number of possible values between any two given values. Continuous variables are typically measured rather than counted.\n",
        "\n",
        "**Characteristics:**\n",
        "*   **Infinite values:** Can take on any real number value.\n",
        "*   **Measurements:** Often result from measurements (e.g., height, weight, temperature, time, speed, age).\n",
        "*   **Decimal or fractional values:** Can have decimal points or fractions.\n",
        "*   **Visual Representation:** Often represented by histograms or line graphs.\n",
        "\n",
        "**Examples:**\n",
        "*   **Height:** A person's height could be 170 cm, 170.5 cm, 170.53 cm, and so on.\n",
        "*   **Temperature:** The temperature outside could be 25°C, 25.1°C, 25.12°C.\n",
        "*   **Time:** The duration of an event, like 3.45 seconds.\n",
        "*   **Weight:** The weight of an object, like 5.67 kg.\n",
        "*   **Income:** A person's annual income, which can be any amount within a range.\n",
        "\n",
        "### 2. Categorical Variables\n",
        "\n",
        "**Definition:** A categorical variable (also known as a qualitative variable) is a variable that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category based on some qualitative property.\n",
        "\n",
        "**Characteristics:**\n",
        "*   **Finite values:** Can only take on a limited number of distinct values or categories.\n",
        "*   **Labels/Names:** Values are often labels or names rather than numbers (though numbers can be used as codes for categories).\n",
        "*   **Counting:** Often result from counting occurrences within categories.\n",
        "*   **Subtypes:** Can be nominal or ordinal.\n",
        "    *   **Nominal:** Categories have no inherent order (e.g., gender, eye color, type of fruit).\n",
        "    *   **Ordinal:** Categories have a meaningful order (e.g., education level (high school, bachelor's, master's), customer satisfaction (poor, fair, good, excellent)).\n",
        "*   **Visual Representation:** Often represented by bar charts or pie charts.\n",
        "\n",
        "**Examples:**\n",
        "*   **Gender:** Male, Female, Non-binary.\n",
        "*   **Eye Color:** Blue, Brown, Green, Hazel.\n",
        "*   **Marital Status:** Single, Married, Divorced, Widowed.\n",
        "*   **Education Level (Ordinal):** High School, Bachelor's Degree, Master's Degree, PhD.\n",
        "*   **Customer Satisfaction (Ordinal):** Very Unsatisfied, Unsatisfied, Neutral, Satisfied, Very Satisfied.\n",
        "*   **Type of Car:** Sedan, SUV, Truck, Hatchback."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. How do we handle categorical variables in Machine Learning? What are the common techniques?**"
      ],
      "metadata": {
        "id": "tinOGtpx16p_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9191e1c2"
      },
      "source": [
        "In machine learning, models typically work with numerical data. Categorical variables, which represent qualitative data, need to be converted into a numerical format before they can be fed into most algorithms. This process is called **encoding**. If not handled properly, categorical variables can lead to misleading results or algorithms failing to train.\n",
        "\n",
        "### Why Handle Categorical Variables?\n",
        "\n",
        "1.  **Algorithm Requirements:** Most machine learning algorithms (e.g., linear regression, support vector machines, neural networks) are designed to work with numerical input.\n",
        "2.  **Order and Magnitude:** Algorithms might incorrectly interpret arbitrary numerical labels as having an ordinal relationship or magnitude if not encoded carefully.\n",
        "3.  **Dimensionality:** Some encoding techniques can increase the dimensionality of the dataset, which needs to be managed.\n",
        "\n",
        "### Common Techniques for Handling Categorical Variables:\n",
        "\n",
        "There are several techniques, and the choice depends on the nature of the categorical variable (nominal vs. ordinal) and the specific machine learning algorithm being used.\n",
        "\n",
        "#### 1. Label Encoding\n",
        "\n",
        "*   **Description:** Assigns a unique integer to each category. For example, if a `Color` variable has categories `Red`, `Green`, `Blue`, it might be encoded as `0`, `1`, `2`.\n",
        "*   **When to Use:** Best suited for **ordinal categorical variables** where there is an inherent order among the categories (e.g., 'Low', 'Medium', 'High' can be encoded as 0, 1, 2). Using it on nominal variables can imply a false order or hierarchy that doesn't exist, which can mislead the model.\n",
        "*   **Pros:** Simple, memory-efficient.\n",
        "*   **Cons:** Implies an arbitrary order or magnitude to nominal data, which can negatively impact model performance.\n",
        "\n",
        "#### 2. One-Hot Encoding\n",
        "\n",
        "*   **Description:** Creates new binary columns (dummy variables) for each category present in the original feature. If a category is present for a given observation, the corresponding new column gets a `1`, and `0` otherwise. For `Color` (Red, Green, Blue), it would create three new columns: `Color_Red`, `Color_Green`, `Color_Blue`.\n",
        "*   **When to Use:** Ideal for **nominal categorical variables** where there is no inherent order. It avoids imposing any artificial order on the data.\n",
        "*   **Pros:** Does not imply order, widely applicable.\n",
        "*   **Cons:** Can significantly increase the dimensionality of the dataset (curse of dimensionality), especially with many categories, which can lead to sparsity and increased training time.\n",
        "\n",
        "#### 3. Ordinal Encoding\n",
        "\n",
        "*   **Description:** Similar to Label Encoding but explicitly used for ordinal variables, where the integer assignment respects the order of the categories. The user explicitly defines the order.\n",
        "*   **When to Use:** When the categorical variable has a clear and meaningful order.\n",
        "*   **Pros:** Preserves the order information, more appropriate than Label Encoding for ordinal data.\n",
        "*   **Cons:** Requires manual mapping of categories to integers.\n",
        "\n",
        "#### 4. Target Encoding (Mean Encoding)\n",
        "\n",
        "*   **Description:** Replaces each category with the mean of the target variable for that category. For example, if a `City` category has an average `Sale` value of $1000, all instances of that `City` will be replaced with $1000.\n",
        "*   **When to Use:** Useful for high cardinality categorical variables (many unique categories) where one-hot encoding would create too many features.\n",
        "*   **Pros:** Reduces dimensionality, captures information about the target variable.\n",
        "*   **Cons:** Prone to overfitting, especially if not regularized. Requires careful cross-validation to avoid data leakage.\n",
        "\n",
        "#### 5. Frequency/Count Encoding\n",
        "\n",
        "*   **Description:** Replaces each category with the count or frequency of its occurrence in the dataset.\n",
        "*   **When to Use:** Can be useful for high cardinality features or when the frequency of a category is believed to be informative.\n",
        "*   **Pros:** Simple, reduces dimensionality.\n",
        "*   **Cons:** Categories with the same frequency will be treated identically, potentially losing information.\n",
        "\n",
        "#### 6. Binary Encoding\n",
        "\n",
        "*   **Description:** First, categories are mapped to integers, then those integers are converted into binary code. Each bit of the binary code creates a new column. For example, `Red` (1) -> `01`, `Green` (2) -> `10`, `Blue` (3) -> `11`.\n",
        "*   **When to Use:** A good compromise between One-Hot Encoding and Label Encoding for high cardinality variables. Reduces dimensionality more than one-hot encoding but retains more information than label encoding for nominal variables.\n",
        "*   **Pros:** Reduces dimensionality compared to one-hot encoding, handles high cardinality.\n",
        "*   **Cons:** Can be less interpretable than one-hot encoding.\n",
        "\n",
        "The choice of encoding method often involves experimentation and depends on the specific dataset, the type of categorical variable, and the machine learning model being employed."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. What do you mean by training and testing a dataset?**"
      ],
      "metadata": {
        "id": "VQaxmB0K2PqY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cce16c98"
      },
      "source": [
        "In machine learning, **training and testing a dataset** refers to the crucial process of dividing your available data into two (or sometimes more) subsets to develop and evaluate a model effectively. This separation is essential to ensure that the model can generalize well to new, unseen data, rather than just memorizing the data it was trained on.\n",
        "\n",
        "### 1. Training a Dataset (Training Phase)\n",
        "\n",
        "**Definition:** The **training dataset** is the subset of the original data used to teach the machine learning model. During this phase, the model learns the underlying patterns, relationships, and features within the data.\n",
        "\n",
        "**Process:**\n",
        "*   **Learning:** The machine learning algorithm processes the training data, adjusting its internal parameters (e.g., weights in a neural network, splits in a decision tree) to minimize a predefined loss function.\n",
        "*   **Pattern Recognition:** The model identifies trends, correlations, and rules from the input features and their corresponding target values (for supervised learning).\n",
        "*   **Model Building:** The outcome of the training phase is a fully developed model that has learned from the data and is ready to make predictions or classifications.\n",
        "\n",
        "**Goal:** To enable the model to learn the most accurate and generalizable representation of the data possible.\n",
        "\n",
        "### 2. Testing a Dataset (Evaluation Phase)\n",
        "\n",
        "**Definition:** The **testing dataset** (also known as the validation or hold-out set, though sometimes a separate validation set is used during training for hyperparameter tuning) is a completely separate subset of the original data that the model has never seen before.\n",
        "\n",
        "**Process:**\n",
        "*   **Unseen Data Evaluation:** After the model has been trained, it is then used to make predictions on the testing dataset.\n",
        "*   **Performance Measurement:** The predictions made by the model on the test data are compared against the actual target values in the test set. Various evaluation metrics (e.g., accuracy, precision, recall, F1-score, Mean Squared Error, R-squared) are then calculated to quantify the model's performance.\n",
        "\n",
        "**Goal:** To assess how well the trained model can generalize to new, unseen data. This provides an unbiased estimate of the model's performance in a real-world scenario.\n",
        "\n",
        "### Why Separate Training and Testing Data?\n",
        "\n",
        "*   **Preventing Overfitting:** If a model is evaluated on the same data it was trained on, it might appear to perform very well simply because it has memorized the training examples (overfitting). This model would then perform poorly on new data. Separating the data ensures that the model's true generalization ability is measured.\n",
        "*   **Unbiased Performance Estimate:** The test set serves as a proxy for new, real-world data, providing an objective measure of how the model will perform once deployed.\n",
        "*   **Model Selection:** When comparing different models or different configurations of the same model, evaluating them on a consistent, unseen test set helps in choosing the best-performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. What is sklearn.preprocessing?**"
      ],
      "metadata": {
        "id": "cigXEUgd2bvK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d59780d"
      },
      "source": [
        "`sklearn.preprocessing` is a module within the popular scikit-learn (Sklearn) Python library that provides a wide range of functions and classes to preprocess raw feature vectors into a suitable format for machine learning algorithms. Data preprocessing is a crucial step in building effective machine learning models because many algorithms are sensitive to the scale and distribution of the input data.\n",
        "\n",
        "### Why is `sklearn.preprocessing` important?\n",
        "\n",
        "1.  **Algorithm Performance:** Many machine learning algorithms perform better or converge faster when input features are scaled or normalized. For example, gradient descent-based algorithms (like Linear Regression, Logistic Regression, Neural Networks, SVMs) often converge much quicker if data is scaled.\n",
        "2.  **Preventing Bias:** Features with larger numerical ranges might disproportionately influence the objective function, leading to a biased model. Scaling helps prevent this.\n",
        "3.  **Handling Categorical Data:** Machine learning models work primarily with numerical data, so categorical features need to be converted into a numerical representation.\n",
        "4.  **Robustness to Outliers:** Some preprocessing techniques can make models more robust to outliers.\n",
        "\n",
        "### Common functionalities in `sklearn.preprocessing`:\n",
        "\n",
        "Here are some of the most frequently used tools available in `sklearn.preprocessing`:\n",
        "\n",
        "1.  **Scaling and Standardization:**\n",
        "    *   **`StandardScaler`**: Standardizes features by removing the mean and scaling to unit variance. The resulting distribution has a mean of 0 and a standard deviation of 1.\n",
        "    *   **`MinMaxScaler`**: Scales features to a given range, typically 0 to 1. This is useful for algorithms that are not robust to large differences in scale.\n",
        "    *   **`MaxAbsScaler`**: Scales each feature by its maximum absolute value. It does not shift/center the data, and thus does not destroy any sparsity.\n",
        "    *   **`RobustScaler`**: Scales features using statistics that are robust to outliers. It removes the median and scales the data according to the Interquartile Range (IQR).\n",
        "\n",
        "2.  **Normalization:**\n",
        "    *   **`Normalizer`**: Normalizes samples individually to unit norm (L1 or L2 norm). This is useful for sparse datasets or when you want to project features onto a unit sphere.\n",
        "\n",
        "3.  **Encoding Categorical Features:**\n",
        "    *   **`LabelEncoder`**: Encodes target labels with values between 0 and `n_classes-1`. Primarily used for encoding target variables.\n",
        "    *   **`OneHotEncoder`**: Encodes categorical features as a one-hot numerical array. This creates a new binary column for each category, which is ideal for nominal (unordered) categorical data.\n",
        "    *   **`OrdinalEncoder`**: Encodes categorical features as an integer array, where each feature is mapped to an integer value based on its order (if specified) or lexicographical order. Suitable for ordinal (ordered) categorical data.\n",
        "\n",
        "4.  **Feature Generation/Transformation:**\n",
        "    *   **`PolynomialFeatures`**: Generates polynomial and interaction features. For example, if you have features `A` and `B`, it can create `A^2`, `B^2`, and `A*B`.\n",
        "\n",
        "5.  **Discretization/Binning:**\n",
        "    *   **`KBinsDiscretizer`**: Transforms numerical features into k bins. This can be useful for algorithms that prefer discrete inputs or for reducing the impact of small fluctuations.\n",
        "\n",
        "### Example Usage Pattern:\n",
        "\n",
        "The typical workflow with `sklearn.preprocessing` involves:\n",
        "1.  Instantiating a preprocessor (e.g., `scaler = StandardScaler()`).\n",
        "2.  Fitting the preprocessor to your training data (`scaler.fit(X_train)`).\n",
        "3.  Transforming your training data (`X_train_scaled = scaler.transform(X_train)`).\n",
        "4.  Transforming your test/new data using the *same fitted* preprocessor (`X_test_scaled = scaler.transform(X_test)`).\n",
        "\n",
        "This ensures that the same scaling/encoding rules learned from the training data are applied consistently to new data, preventing data leakage and ensuring fair evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. What is a Test set?**"
      ],
      "metadata": {
        "id": "H6Nnw4Ey2pBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition: The testing dataset (also known as the validation or hold-out set, though sometimes a separate validation set is used during training for hyperparameter tuning) is a completely separate subset of the original data that the model has never seen before.\n",
        "\n",
        "Process:\n",
        "\n",
        "Unseen Data Evaluation: After the model has been trained, it is then used to make predictions on the testing dataset.\n",
        "Performance Measurement: The predictions made by the model on the test data are compared against the actual target values in the test set. Various evaluation metrics (e.g., accuracy, precision, recall, F1-score, Mean Squared Error, R-squared) are then calculated to quantify the model's performance.\n",
        "Goal: To assess how well the trained model can generalize to new, unseen data. This provides an unbiased estimate of the model's performance in a real-world scenario."
      ],
      "metadata": {
        "id": "sd8wFrpP3SdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?**"
      ],
      "metadata": {
        "id": "cSuDGStn3Yfb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f8c3293"
      },
      "source": [
        "## How to split data for model fitting (training and testing) in Python?\n",
        "\n",
        "In Python, the `sklearn.model_selection` module provides a convenient function called `train_test_split` to divide your dataset into training and testing sets. This is a crucial step to evaluate your model's performance on unseen data.\n",
        "\n",
        "Here's a common way to use it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "10122d2e",
        "outputId": "4571e6c4-a66a-49ac-a5bd-4e3af0e534f5"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Let's create a dummy dataset for demonstration purposes\n",
        "# In a real scenario, you would load your actual dataset here\n",
        "data = {\n",
        "    'feature1': range(100),\n",
        "    'feature2': [i * 2 for i in range(100)],\n",
        "    'target': [0 if i % 2 == 0 else 1 for i in range(100)]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# test_size: The proportion of the dataset to include in the test split.\n",
        "# random_state: Controls the shuffling applied to the data before applying the split.\n",
        "#               Pass an int for reproducible output across multiple function calls.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "\n",
        "display(X_train.head())\n",
        "display(y_train.head())\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (70, 2)\n",
            "Shape of X_test: (30, 2)\n",
            "Shape of y_train: (70,)\n",
            "Shape of y_test: (30,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    feature1  feature2\n",
              "11        11        22\n",
              "47        47        94\n",
              "85        85       170\n",
              "28        28        56\n",
              "93        93       186"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-032198f5-c131-4777-90d3-d4ae26b20f44\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>47</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>85</td>\n",
              "      <td>170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>93</td>\n",
              "      <td>186</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-032198f5-c131-4777-90d3-d4ae26b20f44')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-032198f5-c131-4777-90d3-d4ae26b20f44 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-032198f5-c131-4777-90d3-d4ae26b20f44');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c40846e7-5a8d-4229-82af-2e70427a6c45\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c40846e7-5a8d-4229-82af-2e70427a6c45')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c40846e7-5a8d-4229-82af-2e70427a6c45 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(y_train\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"feature1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35,\n        \"min\": 11,\n        \"max\": 93,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          47,\n          93,\n          85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 71,\n        \"min\": 22,\n        \"max\": 186,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          94,\n          186,\n          170\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "11    1\n",
              "47    1\n",
              "85    1\n",
              "28    0\n",
              "93    1\n",
              "Name: target, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbed919c"
      },
      "source": [
        "## How do you approach a Machine Learning problem?\n",
        "\n",
        "Approaching a Machine Learning problem typically involves a structured methodology to ensure that the problem is well-understood, the data is properly prepared, and the model is robust and performs well. Here's a general outline:\n",
        "\n",
        "### 1. Problem Understanding and Definition\n",
        "*   **Define the Problem:** Clearly articulate what you are trying to solve. Is it a classification, regression, clustering, or another type of problem? What is the goal? What are the business objectives?\n",
        "*   **Identify the Target Variable:** What are you trying to predict or categorize?\n",
        "*   **Determine Evaluation Metrics:** How will you measure the success of your model? (e.g., accuracy, precision, recall, F1-score, RMSE, AUC).\n",
        "*   **Understand Constraints:** Are there any latency requirements, memory limits, interpretability needs, or ethical considerations?\n",
        "\n",
        "### 2. Data Collection and Understanding\n",
        "*   **Gather Data:** Collect relevant data from various sources.\n",
        "*   **Explore Data (EDA):** Perform Exploratory Data Analysis to understand the data's structure, distributions, relationships between variables, and identify potential issues like missing values, outliers, or inconsistencies.\n",
        "*   **Data Validation:** Check data quality, consistency, and completeness.\n",
        "\n",
        "### 3. Data Preprocessing and Feature Engineering\n",
        "*   **Handle Missing Values:** Impute or remove missing data points.\n",
        "*   **Handle Outliers:** Decide whether to remove, transform, or cap outliers.\n",
        "*   **Encode Categorical Variables:** Convert categorical data into numerical formats (e.g., One-Hot Encoding, Label Encoding).\n",
        "*   **Feature Scaling:** Scale numerical features to a standard range (e.g., Standardization, Normalization).\n",
        "*   **Feature Engineering:** Create new features from existing ones to improve model performance or provide more relevant information. This is often an iterative and creative process.\n",
        "*   **Feature Selection:** Select the most relevant features to reduce dimensionality and improve model performance and interpretability.\n",
        "\n",
        "### 4. Model Selection\n",
        "*   **Choose Appropriate Algorithms:** Based on the problem type (classification, regression, etc.) and data characteristics, select one or more candidate machine learning algorithms.\n",
        "*   **Consider Model Complexity:** Balance between simple (interpretable) and complex (potentially higher accuracy) models.\n",
        "\n",
        "### 5. Model Training\n",
        "*   **Split Data:** Divide the preprocessed data into training, validation (optional, for hyperparameter tuning), and test sets.\n",
        "*   **Train the Model:** Fit the chosen algorithm(s) to the training data. The model learns patterns and relationships from this data.\n",
        "\n",
        "### 6. Model Evaluation and Tuning\n",
        "*   **Evaluate on Validation Set:** Use the validation set (if applicable) to tune hyperparameters and assess preliminary performance without touching the test set.\n",
        "*   **Cross-Validation:** Employ techniques like k-fold cross-validation for more robust evaluation and hyperparameter tuning, especially with smaller datasets.\n",
        "*   **Iterate:** Based on evaluation results, iterate on feature engineering, model selection, or hyperparameter tuning.\n",
        "*   **Evaluate on Test Set:** Once you are satisfied with the model's performance on the validation set, evaluate its final performance on the completely unseen test set. This provides an unbiased estimate of how the model will perform in the real world.\n",
        "\n",
        "### 7. Model Deployment and Monitoring\n",
        "*   **Deploy the Model:** Integrate the trained model into a production environment.\n",
        "*   **Monitor Performance:** Continuously monitor the model's performance in production for data drift, concept drift, or performance degradation.\n",
        "*   **Retrain:** Retrain the model periodically with new data to maintain its effectiveness.\n",
        "\n",
        "This iterative process ensures a systematic approach to building effective machine learning solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. Why do we have to perform EDA before fitting a model to the data?**\n"
      ],
      "metadata": {
        "id": "KvIZtlHF3vZF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02f780d2"
      },
      "source": [
        "## Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Exploratory Data Analysis (EDA) is a critical preliminary step in any machine learning project. It involves examining and visualizing datasets to understand their main characteristics, often with visual methods. Performing EDA before model fitting is essential for several key reasons:\n",
        "\n",
        "1.  **Understand the Data Structure and Content:**\n",
        "    *   **Data Types:** Identify the types of variables (numerical, categorical, ordinal, etc.). Incorrect data types can lead to errors or misinterpretations during modeling.\n",
        "    *   **Value Ranges:** Understand the spread and distribution of numerical features (e.g., minimum, maximum, mean, median, standard deviation).\n",
        "    *   **Unique Values:** For categorical features, see how many unique categories exist and their frequencies.\n",
        "\n",
        "2.  **Identify Data Quality Issues:**\n",
        "    *   **Missing Values:** Detect the presence and extent of missing data. EDA helps decide whether to impute them (and how), or drop rows/columns.\n",
        "    *   **Outliers:** Spot extreme values that might be data entry errors or genuine but rare occurrences. Outliers can heavily influence model training and lead to poor performance.\n",
        "    *   **Inconsistent Data:** Find inconsistencies, typos, or incorrect formats that need cleaning.\n",
        "\n",
        "3.  **Uncover Patterns and Relationships:**\n",
        "    *   **Feature Distributions:** Visualize the distribution of individual features (histograms, box plots) to see if they are skewed, multi-modal, or normally distributed. This informs feature transformation choices.\n",
        "    *   **Correlations:** Discover relationships between features and between features and the target variable (scatter plots, correlation matrices). This helps in feature selection and engineering.\n",
        "    *   **Trends/Seasonality:** For time-series data, identify trends, seasonality, and cyclical patterns.\n",
        "\n",
        "4.  **Inform Feature Engineering:**\n",
        "    *   EDA often sparks ideas for creating new, more informative features from existing ones. For example, combining two features, extracting dates, or creating interaction terms.\n",
        "    *   It helps in deciding how to handle categorical variables (e.g., one-hot encoding, label encoding, target encoding) based on their cardinality and relationship with the target.\n",
        "\n",
        "5.  **Guide Model Selection:**\n",
        "    *   Understanding data characteristics (e.g., linearity, non-linearity, presence of outliers, type of target variable) can help narrow down the choice of appropriate machine learning algorithms. For instance, if features are highly correlated, linear models might suffer, suggesting tree-based models could be better.\n",
        "\n",
        "6.  **Detect Data Leakage:**\n",
        "    *   Sometimes, target information might inadvertently be present in features. EDA can help identify such leakage, which would lead to an overly optimistic but unrealistic model performance.\n",
        "\n",
        "7.  **Prevent Overfitting/Underfitting:**\n",
        "    *   By understanding feature distributions and relationships, you can better prepare the data to ensure the model learns generalizable patterns rather than memorizing noise (overfitting) or being too simple to capture the underlying structure (underfitting).\n",
        "\n",
        "In essence, EDA is like getting to know your raw ingredients before you start cooking. Without it, you might end up with an unappetizing or even inedible dish, just as a model built on unexamined data might be inaccurate, biased, or simply ineffective."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12. What is correlation?**"
      ],
      "metadata": {
        "id": "YlVLj6hM3_sq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). It's a common tool for describing simple relationships without making a statement about cause and effect.\n",
        "\n",
        "Key aspects of correlation:\n",
        "\n",
        "Strength: How closely the two variables move together. This is indicated by the absolute value of the correlation coefficient.\n",
        "Direction: Whether the variables tend to increase or decrease together (positive correlation) or whether one variable tends to increase as the other decreases (negative correlation)."
      ],
      "metadata": {
        "id": "9BXH1UUs4Zb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **13. What does negative correlation mean?**"
      ],
      "metadata": {
        "id": "6wFCBYLZ4iuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative correlation describes a relationship between two variables where one increases when the other decreases, and vice versa.\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "When A goes up, B tends to go down\n",
        "\n",
        "When A goes down, B tends to go up\n",
        "\n",
        "Examples:\n",
        "\n",
        "The more time you spend exercising, the less body fat you tend to have.\n",
        "\n",
        "As the price of a product increases, demand often decreases.\n",
        "\n",
        "The hotter the outside temperature, the less heating a house needs."
      ],
      "metadata": {
        "id": "8jYF6D_d4nZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **14. How can you find correlation between variables in Python?**"
      ],
      "metadata": {
        "id": "II6dZ6VV46wg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "b64b8052",
        "outputId": "bbc32ecd-8321-4df0-bea6-c05c264ca6fe"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame, which was created in a previous step.\n",
        "# If you have a different DataFrame, replace 'df' with its name.\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(\"Correlation Matrix:\")\n",
        "display(correlation_matrix)\n",
        "\n",
        "# To find correlation of a specific feature with others (e.g., 'target')\n",
        "print(\"\\nCorrelation of 'target' with other features:\")\n",
        "display(correlation_matrix['target'].sort_values(ascending=False))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation Matrix:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          feature1  feature2    target\n",
              "feature1  1.000000  1.000000  0.017321\n",
              "feature2  1.000000  1.000000  0.017321\n",
              "target    0.017321  0.017321  1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b6eedfc8-d645-4d22-b6c9-a7033b682a0f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>feature1</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.017321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>feature2</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.017321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>0.017321</td>\n",
              "      <td>0.017321</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6eedfc8-d645-4d22-b6c9-a7033b682a0f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b6eedfc8-d645-4d22-b6c9-a7033b682a0f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b6eedfc8-d645-4d22-b6c9-a7033b682a0f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4276d708-3cb2-4b68-acb1-ff659631d8d8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4276d708-3cb2-4b68-acb1-ff659631d8d8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4276d708-3cb2-4b68-acb1-ff659631d8d8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_5ef6dac3-b65b-41ee-bc61-33329b812b9e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('correlation_matrix')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5ef6dac3-b65b-41ee-bc61-33329b812b9e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('correlation_matrix');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "correlation_matrix",
              "summary": "{\n  \"name\": \"correlation_matrix\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"feature1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5673497691521227,\n        \"min\": 0.017321374166049876,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.017321374166049876,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5673497691521227,\n        \"min\": 0.017321374166049876,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.017321374166049876,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5673497691521227,\n        \"min\": 0.017321374166049876,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.017321374166049876\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Correlation of 'target' with other features:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "target      1.000000\n",
              "feature1    0.017321\n",
              "feature2    0.017321\n",
              "Name: target, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>feature1</th>\n",
              "      <td>0.017321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>feature2</th>\n",
              "      <td>0.017321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f78cd55e"
      },
      "source": [
        "You can also visualize the correlation matrix using a heatmap, which provides a clear and intuitive representation of the relationships between variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "938e149c",
        "outputId": "d71b4638-006b-40ee-f168-325a0d1e9d6a"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **15. What is causation? Explain difference between correlation and causation with an example.**"
      ],
      "metadata": {
        "id": "7ExpdtOg5Ouq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0da50d2f"
      },
      "source": [
        "## What is Causation?\n",
        "\n",
        "**Causation** (or causality) means that one event or variable is the direct result of another event or variable. It implies a cause-and-effect relationship, where a change in one variable (the independent variable) directly leads to a change in another variable (the dependent variable).\n",
        "\n",
        "Key aspects of causation:\n",
        "*   **Direct Relationship:** A change in the cause directly produces a change in the effect.\n",
        "*   **Temporal Precedence:** The cause must occur before the effect.\n",
        "*   **Non-spuriousness:** The relationship cannot be explained by other factors.\n",
        "\n",
        "## Difference Between Correlation and Causation\n",
        "\n",
        "While correlation describes a statistical relationship where two variables tend to move together, causation describes a much stronger relationship where one variable *causes* the other.\n",
        "\n",
        "**Correlation:**\n",
        "*   Indicates that two variables are statistically related, meaning they tend to change together.\n",
        "*   Does **not** imply that one causes the other.\n",
        "*   Measured by a correlation coefficient (e.g., Pearson's r).\n",
        "*   Can be positive (both increase/decrease together) or negative (one increases as the other decreases).\n",
        "\n",
        "**Causation:**\n",
        "*   Indicates that a change in one variable directly leads to a change in another.\n",
        "*   Requires evidence of a direct mechanism, temporal order, and ruling out other explanations.\n",
        "*   Often much harder to prove than correlation, typically requiring controlled experiments.\n",
        "\n",
        "### Analogy: Smoke and Fire\n",
        "\n",
        "*   **Correlation:** Where there's smoke, there's often fire. Smoke and fire are highly correlated. You rarely see one without the other.\n",
        "*   **Causation:** Fire *causes* smoke. The fire is the direct reason for the smoke. The smoke does not cause the fire.\n",
        "\n",
        "### Example: Ice Cream Sales and Drowning Incidents\n",
        "\n",
        "Let's consider a classic example:\n",
        "\n",
        "*   **Observation:** As ice cream sales increase, the number of drowning incidents also tends to increase.\n",
        "\n",
        "*   **Correlation:** There is a **positive correlation** between ice cream sales and drowning incidents. When one goes up, the other tends to go up.\n",
        "\n",
        "*   **Causation?** Does eating ice cream cause people to drown? No. Does an increase in drowning cause more ice cream to be sold? No.\n",
        "\n",
        "*   **Underlying Cause (Confounding Variable):** The actual cause for both is a **third variable**: **warm weather**.\n",
        "    *   Warm weather leads to more people buying ice cream.\n",
        "    *   Warm weather also leads to more people swimming, which unfortunately increases the risk of drowning incidents.\n",
        "\n",
        "In this example, ice cream sales and drowning incidents are correlated, but neither causes the other. They are both effects of a common cause (warm weather). This highlights the critical principle: **Correlation does not imply causation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **16. What is an Optimizer? What are different types of optimizers? Explain each with an example.**"
      ],
      "metadata": {
        "id": "XH7JGbhL6Ua2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52d68524"
      },
      "source": [
        "## What is an Optimizer?\n",
        "\n",
        "In the context of machine learning and deep learning, an **optimizer** is an algorithm or function that adjusts the attributes of the neural network, such as weights and learning rate. The goal of an optimizer is to minimize the `loss function` (or maximize the `objective function`) by finding the best possible parameters for the model.\n",
        "\n",
        "Think of it as navigating a landscape with hills and valleys, where the valleys represent lower loss values. The optimizer's job is to find the lowest point in this landscape (the global minimum) as efficiently as possible.\n",
        "\n",
        "Key aspects of optimizers:\n",
        "*   **Loss Function Minimization:** The primary role is to reduce the discrepancy between the model's predictions and the actual target values.\n",
        "*   **Parameter Updates:** They dictate how the model's internal parameters (weights and biases) are updated during training.\n",
        "*   **Learning Rate:** A crucial hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function.\n",
        "\n",
        "## Different Types of Optimizers\n",
        "\n",
        "Optimizers can be broadly categorized into various types, each with its own advantages and disadvantages. Here are some of the most common ones:\n",
        "\n",
        "### 1. Gradient Descent (GD)\n",
        "\n",
        "*   **Explanation:** The most basic optimization algorithm. It calculates the gradient of the loss function with respect to the model's parameters for the *entire training dataset* and updates the parameters in the direction opposite to the gradient. The step size is determined by the learning rate.\n",
        "*   **Pros:** Guaranteed to converge to the global minimum for convex functions and a local minimum for non-convex functions.\n",
        "*   **Cons:** Very slow for large datasets because it processes the entire dataset for each update. Can get stuck in local minima for non-convex functions.\n",
        "*   **Example:** Imagine a hiker trying to get to the lowest point in a valley. With full knowledge of the entire terrain (the whole dataset), they take one big step in the steepest downhill direction.\n",
        "\n",
        "### 2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "*   **Explanation:** Instead of calculating the gradient for the entire dataset, SGD calculates it for a *single randomly chosen training example* at each iteration. This makes updates much faster.\n",
        "*   **Pros:** Much faster than GD, especially for large datasets. Can escape shallow local minima due to its noisy updates.\n",
        "*   **Cons:** Updates are noisy, leading to a zig-zagging path towards the minimum, which can make convergence oscillate. Requires careful tuning of the learning rate.\n",
        "*   **Example:** The hiker now only looks at a small patch of ground right where they are standing and takes a step. This is much faster, but sometimes they might step in a slightly wrong direction before correcting.\n",
        "\n",
        "### 3. Mini-Batch Gradient Descent\n",
        "\n",
        "*   **Explanation:** A compromise between GD and SGD. It calculates the gradient for a small, randomly selected subset (mini-batch) of the training data at each iteration. This combines the speed of SGD with a more stable update direction than pure SGD.\n",
        "*   **Pros:** More stable convergence than SGD. More computationally efficient than GD. Most commonly used approach in deep learning.\n",
        "*   **Cons:** Still requires tuning of the learning rate and batch size.\n",
        "*   **Example:** The hiker looks at a small group of patches around them and takes a step based on the average steepest downhill direction from those patches. This is a good balance of speed and stability.\n",
        "\n",
        "### 4. Momentum\n",
        "\n",
        "*   **Explanation:** Extends SGD by adding a \"momentum\" term to the updates. It helps accelerate SGD in the relevant direction and dampens oscillations. It accumulates an exponentially decaying moving average of past gradients and uses this to determine the next step.\n",
        "*   **Pros:** Helps overcome local minima and navigate flat regions. Smoother convergence than plain SGD.\n",
        "*   **Cons:** Introduces another hyperparameter (momentum coefficient) to tune.\n",
        "*   **Example:** The hiker is now on a skateboard. Even if they hit a small bump (local minimum) or a flat area, the momentum from previous steps helps them keep rolling towards the goal.\n",
        "\n",
        "### 5. Adagrad (Adaptive Gradient Algorithm)\n",
        "\n",
        "*   **Explanation:** Adapts the learning rate for each parameter individually based on the past gradients. It gives larger updates for infrequent parameters and smaller updates for frequent parameters. It squares the gradients and accumulates them, then divides the learning rate by the square root of this accumulated sum.\n",
        "*   **Pros:** Excellent for sparse data (e.g., natural language processing) where some features occur more often than others.\n",
        "*   **Cons:** The learning rate can become very small over time, leading to premature stopping of learning.\n",
        "*   **Example:** The hiker has a special map that tells them how bumpy the terrain has been in different directions. They take larger steps in areas that have been consistently smooth and smaller steps where it has been very bumpy.\n",
        "\n",
        "### 6. RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "*   **Explanation:** Addresses Adagrad's aggressively diminishing learning rates. It uses a moving average of squared gradients instead of accumulating all past squared gradients, preventing the learning rate from shrinking too quickly.\n",
        "*   **Pros:** Performs well on non-stationary objectives (where the properties of the loss function change over time).\n",
        "*   **Cons:** Still requires manual tuning of the learning rate and decay rate.\n",
        "*   **Example:** Similar to Adagrad's special map, but this map 'forgets' old bumpiness quickly, so the hiker doesn't get stuck in a cautious slow pace forever.\n",
        "\n",
        "### 7. Adam (Adaptive Moment Estimation)\n",
        "\n",
        "*   **Explanation:** Combines the best aspects of Momentum and RMSprop. It calculates an exponentially decaying average of past gradients (like Momentum) and an exponentially decaying average of past squared gradients (like RMSprop). It then uses both to adapt the learning rate for each parameter.\n",
        "*   **Pros:** Generally considered one of the best default optimizers. Efficient, requires less memory, and performs well in a wide range of problems.\n",
        "*   **Cons:** Can sometimes generalize poorly on certain tasks compared to SGD with momentum, especially in the later stages of training.\n",
        "*   **Example:** The hiker now has a smart drone that not only tracks their speed and direction (momentum) but also keeps a real-time record of the terrain's recent roughness (RMSprop), allowing for highly adaptive and efficient pathfinding.\n",
        "\n",
        "Choosing the right optimizer often involves experimentation, but Adam is a popular starting point due to its robustness and efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **17. What is sklearn.linear_model ?**"
      ],
      "metadata": {
        "id": "4dtfOT0H600R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc789fce"
      },
      "source": [
        "## What is `sklearn.linear_model`?\n",
        "\n",
        "`sklearn.linear_model` is a crucial module within the `scikit-learn` (Sklearn) Python library. It provides a wide array of tools and algorithms specifically designed for building **linear models**.\n",
        "\n",
        "Linear models are a class of models that assume a linear relationship between the input features (`X`) and the target variable (`y`). Despite their simplicity, linear models are fundamental in machine learning due to their interpretability, efficiency, and effectiveness for many real-world problems.\n",
        "\n",
        "### Key Concepts of `sklearn.linear_model`:\n",
        "\n",
        "1.  **Linear Relationships:** These models aim to find a linear function that best describes the relationship between the features and the target. For example, in regression, it tries to find a line or a hyperplane that best fits the data points.\n",
        "\n",
        "2.  **Interpretability:** Linear models are often highly interpretable. The coefficients assigned to each feature can tell you the strength and direction of that feature's influence on the target variable.\n",
        "\n",
        "3.  **Efficiency:** They are generally computationally efficient to train and make predictions, making them suitable for large datasets and scenarios where speed is critical.\n",
        "\n",
        "4.  **Baseline Models:** Linear models often serve as excellent baseline models against which more complex non-linear models can be compared.\n",
        "\n",
        "### Common Estimators (Algorithms) in `sklearn.linear_model`:\n",
        "\n",
        "This module includes various types of linear models for both **regression** and **classification** tasks, often with different regularization techniques to prevent overfitting.\n",
        "\n",
        "#### For Regression:\n",
        "\n",
        "*   **`LinearRegression`**: The most basic form of linear regression, which fits a linear model with coefficients `w = (X^T X)^(-1) X^T y` to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
        "\n",
        "    *   **Example Use Case:** Predicting house prices based on features like size, number of bedrooms, and location.\n",
        "\n",
        "*   **`Ridge`**: Implements Ridge regression, which adds L2 regularization to the ordinary least squares cost function. This helps prevent overfitting by shrinking the coefficients towards zero.\n",
        "\n",
        "    *   **Example Use Case:** When you have many features and suspect multicollinearity, Ridge can provide more stable coefficient estimates.\n",
        "\n",
        "*   **`Lasso`**: Implements Lasso regression, which adds L1 regularization. This has the effect of driving some coefficients exactly to zero, effectively performing feature selection.\n",
        "\n",
        "    *   **Example Use Case:** When you want to select a subset of important features from a larger set.\n",
        "\n",
        "*   **`ElasticNet`**: Combines L1 and L2 regularization, offering a balance between Ridge and Lasso. It's particularly useful when there are multiple correlated features.\n",
        "\n",
        "    *   **Example Use Case:** A general-purpose alternative to Ridge or Lasso when you're unsure which regularization works best.\n",
        "\n",
        "#### For Classification:\n",
        "\n",
        "*   **`LogisticRegression`**: Despite its name, this is a linear model for **classification**, not regression. It models the probability that a given input belongs to a certain class using a logistic (sigmoid) function.\n",
        "\n",
        "    *   **Example Use Case:** Predicting whether an email is spam (binary classification) or categorizing customer reviews into positive, neutral, or negative (multi-class classification).\n",
        "\n",
        "*   **`SGDClassifier`**: Implements linear classifiers (like SVM, Logistic Regression) with Stochastic Gradient Descent (SGD) training. It's efficient for very large datasets.\n",
        "\n",
        "    *   **Example Use Case:** Training a linear classifier on datasets with millions of samples when other solvers are too slow.\n",
        "\n",
        "*   **`Perceptron`**: A simple linear classifier often used as an introduction to neural networks. It learns a decision boundary by iterating through the training data.\n",
        "\n",
        "    *   **Example Use Case:** Simple binary classification tasks, often for educational purposes.\n",
        "\n",
        "### General Workflow:\n",
        "\n",
        "The general workflow for using models in `sklearn.linear_model` is consistent with other scikit-learn estimators:\n",
        "\n",
        "1.  **Import** the desired model (e.g., `from sklearn.linear_model import LinearRegression`).\n",
        "2.  **Instantiate** the model (e.g., `model = LinearRegression()`).\n",
        "3.  **Train** the model using the `fit()` method on your training data (e.g., `model.fit(X_train, y_train)`).\n",
        "4.  **Make predictions** using the `predict()` method (e.g., `predictions = model.predict(X_test)`).\n",
        "5.  **Evaluate** the model's performance using appropriate metrics.\n",
        "\n",
        "In summary, `sklearn.linear_model` is a fundamental and versatile module providing essential algorithms for building powerful and interpretable linear models in machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **18. What does model.fit() do? What arguments must be given?**"
      ],
      "metadata": {
        "id": "7WNzmBxE7CJo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6b652b6"
      },
      "source": [
        "## What does `model.fit()` do?\n",
        "\n",
        "In machine learning, the `fit()` method is a core function used across various libraries (like scikit-learn, TensorFlow, Keras) to **train a model** on your dataset. It's the process where the machine learning algorithm learns patterns and relationships from the provided data.\n",
        "\n",
        "Essentially, when you call `model.fit()`:\n",
        "\n",
        "1.  **Learning:** The algorithm takes the input features (data `X`) and their corresponding target values (labels `y`) and attempts to find a mathematical relationship or pattern between them.\n",
        "2.  **Parameter Adjustment:** During this process, the model's internal parameters (e.g., coefficients in linear regression, weights and biases in neural networks, split points in decision trees) are iteratively adjusted to minimize a defined loss function or optimize an objective function.\n",
        "3.  **Model Building:** By the end of the `fit()` operation, the model has been 'trained' and is now ready to make predictions on new, unseen data.\n",
        "\n",
        "Think of it as a student learning from textbooks and exercises. The `fit()` method is the study time, where the model \"reads\" the data and \"practices\" to understand the underlying concepts.\n",
        "\n",
        "## What arguments must be given?\n",
        "\n",
        "The `model.fit()` method typically requires at least two primary arguments:\n",
        "\n",
        "1.  **`X` (or `features`, `inputs`)**: This argument represents the **training data features**. It should be a data structure (commonly a Pandas DataFrame or a NumPy array) containing the independent variables (features) that the model will use to learn and make predictions. Each row usually represents an observation, and each column represents a feature.\n",
        "\n",
        "2.  **`y` (or `target`, `labels`, `outputs`)**: This argument represents the **training data target variable**. It should be a data structure (commonly a Pandas Series or a NumPy array) containing the dependent variable (the outcome or label) that the model is trying to predict. It must align with the rows of `X`.\n",
        "\n",
        "### Example (using scikit-learn):\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Assuming you have your data loaded into X (features) and y (target)\n",
        "# For demonstration, let's use the dummy data from earlier\n",
        "X = df[['feature1', 'feature2']] # Your feature DataFrame\n",
        "y = df['target'] # Your target Series\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Instantiate a model (e.g., Logistic Regression)\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model using the fit() method\n",
        "# X_train are the features for training\n",
        "# y_train are the corresponding target values for training\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training complete!\")\n",
        "```\n",
        "\n",
        "### Optional Arguments:\n",
        "\n",
        "Many `fit()` methods can also accept optional arguments, depending on the specific model and library. Common optional arguments include:\n",
        "\n",
        "*   **`sample_weight`**: An array of weights that are assigned to individual samples. This can be useful for handling imbalanced datasets or giving more importance to certain samples.\n",
        "*   **`validation_data`**: In deep learning frameworks like Keras, this argument allows you to provide a separate validation dataset (`(X_val, y_val)`) during training. The model's performance will be evaluated on this data after each epoch, which helps monitor for overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **19. What does model.predict() do? What arguments must be given?**"
      ],
      "metadata": {
        "id": "keMI1ReD7Og8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf34a3d3"
      },
      "source": [
        "## What does `model.predict()` do?\n",
        "\n",
        "After a machine learning model has been trained using the `model.fit()` method, the `model.predict()` method is used to **generate predictions** on new, unseen data.\n",
        "\n",
        "Essentially, when you call `model.predict()`:\n",
        "\n",
        "1.  **Input Data:** The trained model takes new input features (`X_new`) that it has not encountered during training.\n",
        "2.  **Prediction Generation:** Based on the patterns and relationships it learned during the fitting process, the model applies its internal logic to these new inputs to produce an output (a prediction).\n",
        "3.  **Output Type:** The type of output depends on the task:\n",
        "    *   For **regression** problems, `predict()` will return a numerical value (e.g., a predicted house price, temperature).\n",
        "    *   For **classification** problems, `predict()` will typically return the predicted class label (e.g., 'spam' or 'not spam', 'cat' or 'dog'). Many classifiers also have `predict_proba()` to return the probabilities of each class.\n",
        "\n",
        "Think of it as the student (the model) who has finished studying (fitting) and is now taking a test (predicting on new data) to apply what they've learned.\n",
        "\n",
        "## What arguments must be given?\n",
        "\n",
        "The `model.predict()` method typically requires one primary argument:\n",
        "\n",
        "1.  **`X` (or `features`, `inputs`)**: This argument represents the **new data features** on which you want the model to make predictions. It should be a data structure (commonly a Pandas DataFrame or a NumPy array) containing the independent variables (features) for which you want to predict the target. Crucially, the structure and number of features in this `X` argument must match the features that the model was trained on.\n",
        "\n",
        "### Example (using scikit-learn):\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined from previous steps\n",
        "# (e.g., from the example provided for model.fit())\n",
        "\n",
        "# Instantiate and train a model\n",
        "model = LogisticRegression(random_state=42) # Added random_state for reproducibility\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Use the trained model to make predictions on the test set\n",
        "# X_test contains the features of the unseen data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Predictions on the test set:\")\n",
        "print(y_pred)\n",
        "\n",
        "# You can also predict on a new single sample (ensure it's in the correct shape, e.g., 2D array)\n",
        "# new_sample = pd.DataFrame([[50, 100]], columns=['feature1', 'feature2'])\n",
        "# new_prediction = model.predict(new_sample)\n",
        "# print(f\"\\nPrediction for a new sample: {new_prediction[0]}\")\n",
        "\n",
        "print(\"\\nFirst 5 actual target values from y_test:\")\n",
        "print(y_test.head())\n",
        "\n",
        "print(\"\\nFirst 5 predicted target values from y_pred:\")\n",
        "print(y_pred[:5])\n",
        "```\n",
        "\n",
        "### Important Considerations:\n",
        "\n",
        "*   **Data Format:** The input data for `predict()` must have the same number of features and be in the same format (e.g., scaled, encoded) as the data used for training the model.\n",
        "*   **New Data Only:** Always use `predict()` on data that the model has *not* seen during training to get an unbiased estimate of its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **20. What are continuous and categorical variables?**"
      ],
      "metadata": {
        "id": "wPiimryI7ZJ_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e479f8c3"
      },
      "source": [
        "In statistics and data analysis, variables are generally classified into two main types based on the nature of the data they represent:\n",
        "\n",
        "### 1. Continuous Variables\n",
        "\n",
        "**Definition:** A continuous variable is a variable that can take any value within a given range. This means there are an infinite number of possible values between any two given values. Continuous variables are typically measured rather than counted.\n",
        "\n",
        "**Characteristics:**\n",
        "*   **Infinite values:** Can take on any real number value.\n",
        "*   **Measurements:** Often result from measurements (e.g., height, weight, temperature, time, speed, age).\n",
        "*   **Decimal or fractional values:** Can have decimal points or fractions.\n",
        "*   **Visual Representation:** Often represented by histograms or line graphs.\n",
        "\n",
        "**Examples:**\n",
        "*   **Height:** A person's height could be 170 cm, 170.5 cm, 170.53 cm, and so on.\n",
        "*   **Temperature:** The temperature outside could be 25°C, 25.1°C, 25.12°C.\n",
        "*   **Time:** The duration of an event, like 3.45 seconds.\n",
        "*   **Weight:** The weight of an object, like 5.67 kg.\n",
        "*   **Income:** A person's annual income, which can be any amount within a range.\n",
        "\n",
        "### 2. Categorical Variables\n",
        "\n",
        "**Definition:** A categorical variable (also known as a qualitative variable) is a variable that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category based on some qualitative property.\n",
        "\n",
        "**Characteristics:**\n",
        "*   **Finite values:** Can only take on a limited number of distinct values or categories.\n",
        "*   **Labels/Names:** Values are often labels or names rather than numbers (though numbers can be used as codes for categories).\n",
        "*   **Counting:** Often result from counting occurrences within categories.\n",
        "*   **Subtypes:** Can be nominal or ordinal.\n",
        "    *   **Nominal:** Categories have no inherent order (e.g., gender, eye color, type of fruit).\n",
        "    *   **Ordinal:** Categories have a meaningful order (e.g., education level (high school, bachelor's, master's), customer satisfaction (poor, fair, good, excellent)).\n",
        "*   **Visual Representation:** Often represented by bar charts or pie charts.\n",
        "\n",
        "**Examples:**\n",
        "*   **Gender:** Male, Female, Non-binary.\n",
        "*   **Eye Color:** Blue, Brown, Green, Hazel.\n",
        "*   **Marital Status:** Single, Married, Divorced, Widowed.\n",
        "*   **Education Level (Ordinal):** High School, Bachelor's Degree, Master's Degree, PhD.\n",
        "*   **Customer Satisfaction (Ordinal):** Very Unsatisfied, Unsatisfied, Neutral, Satisfied, Very Satisfied.\n",
        "*   **Type of Car:** Sedan, SUV, Truck, Hatchback."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **21. What is feature scaling? How does it help in Machine Learning?**"
      ],
      "metadata": {
        "id": "tgsBGEqg7mLm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5835b0a"
      },
      "source": [
        "## What is Feature Scaling?\n",
        "\n",
        "**Feature scaling** is a data preprocessing technique used to standardize or normalize the range of independent variables (features) in a dataset. In simpler terms, it adjusts the numerical values of different features to a common scale, without distorting differences in the ranges of values or losing information.\n",
        "\n",
        "Imagine you have a dataset with two features: `age` (ranging from 18 to 80) and `income` (ranging from $20,000 to $200,000). Without scaling, the `income` feature, having a much larger range, would disproportionately influence distance-based machine learning algorithms.\n",
        "\n",
        "### Common Feature Scaling Techniques:\n",
        "\n",
        "There are two main types of feature scaling:\n",
        "\n",
        "1.  **Standardization (Z-score Normalization):**\n",
        "    *   **Method:** Transforms data to have a mean of 0 and a standard deviation of 1. Each value is transformed using the formula: `z = (x - μ) / σ`, where `μ` is the mean and `σ` is the standard deviation of the feature.\n",
        "    *   **When to use:** Suitable for algorithms that assume a Gaussian distribution of the data (e.g., Linear Regression, Logistic Regression, LDA) or algorithms that are sensitive to the absolute magnitude of features (e.g., SVM, K-Means, Neural Networks).\n",
        "    *   **Effect:** Handles outliers well as it doesn't bound the values to a specific range.\n",
        "\n",
        "2.  **Normalization (Min-Max Scaling):**\n",
        "    *   **Method:** Scales features to a fixed range, usually between 0 and 1. The formula is: `x' = (x - min(x)) / (max(x) - min(x))`.\n",
        "    *   **When to use:** Useful for algorithms that don't assume any distribution of the data (e.g., K-Nearest Neighbors, Neural Networks with sigmoid/tanh activation functions) or when you want to bound your features to a specific range.\n",
        "    *   **Effect:** Sensitive to outliers, as they can disproportionately affect the min and max values, compressing the range of other data points.\n",
        "\n",
        "## How does it help in Machine Learning?\n",
        "\n",
        "Feature scaling is a critical preprocessing step for many machine learning algorithms due to several reasons:\n",
        "\n",
        "1.  **Prevents Dominance by Features with Larger Magnitudes:**\n",
        "    *   **Problem:** If features have vastly different ranges, the feature with the largest range might dominate the cost function, regardless of its actual importance. Algorithms would be more influenced by the feature with higher values.\n",
        "    *   **Solution:** Scaling ensures that all features contribute equally to the distance calculations or gradient descent steps, preventing features with larger magnitudes from overpowering those with smaller ones.\n",
        "\n",
        "2.  **Improves Performance of Distance-Based Algorithms:**\n",
        "    *   **Algorithms:** K-Nearest Neighbors (KNN), Support Vector Machines (SVMs), K-Means clustering, etc., calculate distances between data points. Without scaling, features with larger values would have a much greater impact on the distance calculation.\n",
        "    *   **Benefit:** Scaling ensures that distance metrics accurately reflect the similarity between observations across all features.\n",
        "\n",
        "3.  **Accelerates Convergence of Gradient-Based Optimization Algorithms:**\n",
        "    *   **Algorithms:** Gradient Descent (used in Linear Regression, Logistic Regression, Neural Networks) aims to find the minimum of a cost function.\n",
        "    *   **Problem:** If features are on different scales, the cost function will be elongated and narrow, leading to a zig-zagging path for gradient descent, which slows down convergence or might even get stuck in local minima.\n",
        "    *   **Benefit:** Scaling makes the cost function more spherical, allowing gradient descent to converge much faster and more stably, as it can take more direct steps towards the minimum.\n",
        "\n",
        "4.  **Helps Regularization Techniques:**\n",
        "    *   **Techniques:** L1 (Lasso) and L2 (Ridge) regularization penalize large coefficients. If features are not scaled, larger coefficients might be assigned to features with smaller scales to compensate, leading to an unfair penalty or selection bias.\n",
        "    *   **Benefit:** Scaling ensures that the regularization penalty is applied fairly across all features, promoting a more effective regularization.\n",
        "\n",
        "5.  **Improves Model Interpretability (in some cases):**\n",
        "    *   For models like Linear Regression, scaled features can make the coefficients more directly comparable, allowing for a better understanding of each feature's relative importance.\n",
        "\n",
        "In summary, feature scaling is not always strictly necessary for all algorithms (e.g., tree-based models like Decision Trees, Random Forests, Gradient Boosting Machines are generally not sensitive to feature scales), but it is a crucial step for a wide range of algorithms to ensure optimal performance, faster convergence, and fair treatment of all features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **22. How do we perform scaling in Python?**"
      ],
      "metadata": {
        "id": "ZZ3OBjBz7zSC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d424c814"
      },
      "source": [
        "## How to perform scaling in Python?\n",
        "\n",
        "In Python, the `sklearn.preprocessing` module provides efficient tools for feature scaling. The most commonly used are `StandardScaler` for standardization and `MinMaxScaler` for normalization.\n",
        "\n",
        "It's crucial to fit the scaler only on the training data (`X_train`) to prevent data leakage. Then, this fitted scaler is used to transform both the training data and the test data (`X_test`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6671b775",
        "outputId": "19f6dadd-1a58-479c-9504-76d96029bfc9"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X_train and X_test are already defined from previous steps\n",
        "# (e.g., from the example provided for model.fit())\n",
        "\n",
        "print(\"Original X_train head:\\n\", X_train.head())\n",
        "print(\"Original X_test head:\\n\", X_test.head())\n",
        "\n",
        "# 1. Standardization (Z-score Normalization)\n",
        "scaler_standard = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data ONLY\n",
        "scaler_standard.fit(X_train)\n",
        "\n",
        "# Transform both training and test data\n",
        "X_train_scaled_standard = scaler_standard.transform(X_train)\n",
        "X_test_scaled_standard = scaler_standard.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for better readability (optional)\n",
        "X_train_scaled_standard_df = pd.DataFrame(X_train_scaled_standard, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled_standard_df = pd.DataFrame(X_test_scaled_standard, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(\"\\n--- Standardization ---\")\n",
        "print(\"X_train_scaled (StandardScaler) head:\\n\", X_train_scaled_standard_df.head())\n",
        "print(\"X_test_scaled (StandardScaler) head:\\n\", X_test_scaled_standard_df.head())\n",
        "\n",
        "# Verify mean (should be ~0) and std (should be ~1) for training data\n",
        "print(\"\\nMean of X_train_scaled (StandardScaler):\\n\", X_train_scaled_standard_df.mean())\n",
        "print(\"Standard Deviation of X_train_scaled (StandardScaler):\\n\", X_train_scaled_standard_df.std())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original X_train head:\n",
            "     feature1  feature2\n",
            "11        11        22\n",
            "47        47        94\n",
            "85        85       170\n",
            "28        28        56\n",
            "93        93       186\n",
            "Original X_test head:\n",
            "     feature1  feature2\n",
            "83        83       166\n",
            "53        53       106\n",
            "70        70       140\n",
            "45        45        90\n",
            "44        44        88\n",
            "\n",
            "--- Standardization ---\n",
            "X_train_scaled (StandardScaler) head:\n",
            "     feature1  feature2\n",
            "11 -1.371516 -1.371516\n",
            "47 -0.127376 -0.127376\n",
            "85  1.185882  1.185882\n",
            "28 -0.784005 -0.784005\n",
            "93  1.462358  1.462358\n",
            "X_test_scaled (StandardScaler) head:\n",
            "     feature1  feature2\n",
            "83  1.116763  1.116763\n",
            "53  0.079980  0.079980\n",
            "70  0.667491  0.667491\n",
            "45 -0.196495 -0.196495\n",
            "44 -0.231054 -0.231054\n",
            "\n",
            "Mean of X_train_scaled (StandardScaler):\n",
            " feature1    9.119689e-17\n",
            "feature2    9.119689e-17\n",
            "dtype: float64\n",
            "Standard Deviation of X_train_scaled (StandardScaler):\n",
            " feature1    1.00722\n",
            "feature2    1.00722\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d36eba0d",
        "outputId": "64ddec28-8ea8-431a-99f0-85eca3e24b83"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X_train and X_test are already defined\n",
        "\n",
        "# 2. Normalization (Min-Max Scaling)\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the training data ONLY\n",
        "scaler_minmax.fit(X_train)\n",
        "\n",
        "# Transform both training and test data\n",
        "X_train_scaled_minmax = scaler_minmax.transform(X_train)\n",
        "X_test_scaled_minmax = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for better readability (optional)\n",
        "X_train_scaled_minmax_df = pd.DataFrame(X_train_scaled_minmax, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled_minmax_df = pd.DataFrame(X_test_scaled_minmax, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(\"\\n--- Min-Max Scaling ---\")\n",
        "print(\"X_train_scaled (MinMaxScaler) head:\\n\", X_train_scaled_minmax_df.head())\n",
        "print(\"X_test_scaled (MinMaxScaler) head:\\n\", X_test_scaled_minmax_df.head())\n",
        "\n",
        "# Verify min (should be ~0) and max (should be ~1) for training data\n",
        "print(\"\\nMin of X_train_scaled (MinMaxScaler):\\n\", X_train_scaled_minmax_df.min())\n",
        "print(\"Max of X_train_scaled (MinMaxScaler):\\n\", X_train_scaled_minmax_df.max())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Min-Max Scaling ---\n",
            "X_train_scaled (MinMaxScaler) head:\n",
            "     feature1  feature2\n",
            "11  0.102041  0.102041\n",
            "47  0.469388  0.469388\n",
            "85  0.857143  0.857143\n",
            "28  0.275510  0.275510\n",
            "93  0.938776  0.938776\n",
            "X_test_scaled (MinMaxScaler) head:\n",
            "     feature1  feature2\n",
            "83  0.836735  0.836735\n",
            "53  0.530612  0.530612\n",
            "70  0.704082  0.704082\n",
            "45  0.448980  0.448980\n",
            "44  0.438776  0.438776\n",
            "\n",
            "Min of X_train_scaled (MinMaxScaler):\n",
            " feature1    0.0\n",
            "feature2    0.0\n",
            "dtype: float64\n",
            "Max of X_train_scaled (MinMaxScaler):\n",
            " feature1    1.0\n",
            "feature2    1.0\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **23. What is sklearn.preprocessing?**"
      ],
      "metadata": {
        "id": "veQMpNea8Doc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09027b13"
      },
      "source": [
        "`sklearn.preprocessing` is a module within the popular scikit-learn (Sklearn) Python library that provides a wide range of functions and classes to preprocess raw feature vectors into a suitable format for machine learning algorithms. Data preprocessing is a crucial step in building effective machine learning models because many algorithms are sensitive to the scale and distribution of the input data.\n",
        "\n",
        "### Why is `sklearn.preprocessing` important?\n",
        "\n",
        "1.  **Algorithm Performance:** Many machine learning algorithms perform better or converge faster when input features are scaled or normalized. For example, gradient descent-based algorithms (like Linear Regression, Logistic Regression, Neural Networks, SVMs) often converge much quicker if data is scaled.\n",
        "2.  **Preventing Bias:** Features with larger numerical ranges might disproportionately influence the objective function, leading to a biased model. Scaling helps prevent this.\n",
        "3.  **Handling Categorical Data:** Machine learning models work primarily with numerical data, so categorical features need to be converted into a numerical representation.\n",
        "4.  **Robustness to Outliers:** Some preprocessing techniques can make models more robust to outliers.\n",
        "\n",
        "### Common functionalities in `sklearn.preprocessing`:\n",
        "\n",
        "Here are some of the most frequently used tools available in `sklearn.preprocessing`:\n",
        "\n",
        "1.  **Scaling and Standardization:**\n",
        "    *   **`StandardScaler`**: Standardizes features by removing the mean and scaling to unit variance. The resulting distribution has a mean of 0 and a standard deviation of 1.\n",
        "    *   **`MinMaxScaler`**: Scales features to a given range, typically 0 to 1. This is useful for algorithms that are not robust to large differences in scale.\n",
        "    *   **`MaxAbsScaler`**: Scales each feature by its maximum absolute value. It does not shift/center the data, and thus does not destroy any sparsity.\n",
        "    *   **`RobustScaler`**: Scales features using statistics that are robust to outliers. It removes the median and scales the data according to the Interquartile Range (IQR).\n",
        "\n",
        "2.  **Normalization:**\n",
        "    *   **`Normalizer`**: Normalizes samples individually to unit norm (L1 or L2 norm). This is useful for sparse datasets or when you want to project features onto a unit sphere.\n",
        "\n",
        "3.  **Encoding Categorical Features:**\n",
        "    *   **`LabelEncoder`**: Encodes target labels with values between 0 and `n_classes-1`. Primarily used for encoding target variables.\n",
        "    *   **`OneHotEncoder`**: Encodes categorical features as a one-hot numerical array. This creates a new binary column for each category, which is ideal for nominal (unordered) categorical data.\n",
        "    *   **`OrdinalEncoder`**: Encodes categorical features as an integer array, where each feature is mapped to an integer value based on its order (if specified) or lexicographical order. Suitable for ordinal (ordered) categorical data.\n",
        "\n",
        "4.  **Feature Generation/Transformation:**\n",
        "    *   **`PolynomialFeatures`**: Generates polynomial and interaction features. For example, if you have features `A` and `B`, it can create `A^2`, `B^2`, and `A*B`.\n",
        "\n",
        "5.  **Discretization/Binning:**\n",
        "    *   **`KBinsDiscretizer`**: Transforms numerical features into k bins. This can be useful for algorithms that prefer discrete inputs or for reducing the impact of small fluctuations.\n",
        "\n",
        "### Example Usage Pattern:\n",
        "\n",
        "The typical workflow with `sklearn.preprocessing` involves:\n",
        "1.  Instantiating a preprocessor (e.g., `scaler = StandardScaler()`).\n",
        "2.  Fitting the preprocessor to your training data (`scaler.fit(X_train)`).\n",
        "3.  Transforming your training data (`X_train_scaled = scaler.transform(X_train)`).\n",
        "4.  Transforming your test/new data using the *same fitted* preprocessor (`X_test_scaled = scaler.transform(X_test)`).\n",
        "\n",
        "This ensures that the same scaling/encoding rules learned from the training data are applied consistently to new data, preventing data leakage and ensuring fair evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **24. How do we split data for model fitting (training and testing) in Python?**"
      ],
      "metadata": {
        "id": "vwaoQwtf8P0H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aefd6fd1"
      },
      "source": [
        "## How to split data for model fitting (training and testing) in Python?\n",
        "\n",
        "In Python, the `sklearn.model_selection` module provides a convenient function called `train_test_split` to divide your dataset into training and testing sets. This is a crucial step to evaluate your model's performance on unseen data.\n",
        "\n",
        "Here's a common way to use it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "19f17379",
        "outputId": "c3fc295e-f41e-4cf0-a9f3-28d4ccd00c9a"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Let's create a dummy dataset for demonstration purposes\n",
        "# In a real scenario, you would load your actual dataset here\n",
        "data = {\n",
        "    'feature1': range(100),\n",
        "    'feature2': [i * 2 for i in range(100)],\n",
        "    'target': [0 if i % 2 == 0 else 1 for i in range(100)]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# test_size: The proportion of the dataset to include in the test split.\n",
        "# random_state: Controls the shuffling applied to the data before applying the split.\n",
        "#               Pass an int for reproducible output across multiple function calls.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "\n",
        "display(X_train.head())\n",
        "display(y_train.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (70, 2)\n",
            "Shape of X_test: (30, 2)\n",
            "Shape of y_train: (70,)\n",
            "Shape of y_test: (30,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    feature1  feature2\n",
              "11        11        22\n",
              "47        47        94\n",
              "85        85       170\n",
              "28        28        56\n",
              "93        93       186"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a989d23d-de2d-42be-8e09-7920e3441b2e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>47</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>85</td>\n",
              "      <td>170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>93</td>\n",
              "      <td>186</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a989d23d-de2d-42be-8e09-7920e3441b2e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a989d23d-de2d-42be-8e09-7920e3441b2e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a989d23d-de2d-42be-8e09-7920e3441b2e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ebf493b0-6369-4055-b670-30385fa691b3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ebf493b0-6369-4055-b670-30385fa691b3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ebf493b0-6369-4055-b670-30385fa691b3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(y_train\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"feature1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35,\n        \"min\": 11,\n        \"max\": 93,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          47,\n          93,\n          85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 71,\n        \"min\": 22,\n        \"max\": 186,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          94,\n          186,\n          170\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "11    1\n",
              "47    1\n",
              "85    1\n",
              "28    0\n",
              "93    1\n",
              "Name: target, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **25. Explain data encoding?**"
      ],
      "metadata": {
        "id": "9QhwXpHW8fGF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57e7b5c0"
      },
      "source": [
        "In machine learning, models typically work with numerical data. Categorical variables, which represent qualitative data, need to be converted into a numerical format before they can be fed into most algorithms. This process is called **encoding**. If not handled properly, categorical variables can lead to misleading results or algorithms failing to train.\n",
        "\n",
        "### Why Handle Categorical Variables?\n",
        "\n",
        "1.  **Algorithm Requirements:** Most machine learning algorithms (e.g., linear regression, support vector machines, neural networks) are designed to work with numerical input.\n",
        "2.  **Order and Magnitude:** Algorithms might incorrectly interpret arbitrary numerical labels as having an ordinal relationship or magnitude if not encoded carefully.\n",
        "3.  **Dimensionality:** Some encoding techniques can increase the dimensionality of the dataset, which needs to be managed.\n",
        "\n",
        "### Common Techniques for Handling Categorical Variables:\n",
        "\n",
        "There are several techniques, and the choice depends on the nature of the categorical variable (nominal vs. ordinal) and the specific machine learning algorithm being used.\n",
        "\n",
        "#### 1. Label Encoding\n",
        "\n",
        "*   **Description:** Assigns a unique integer to each category. For example, if a `Color` variable has categories `Red`, `Green`, `Blue`, it might be encoded as `0`, `1`, `2`.\n",
        "*   **When to Use:** Best suited for **ordinal categorical variables** where there is an inherent order among the categories (e.g., 'Low', 'Medium', 'High' can be encoded as 0, 1, 2). Using it on nominal variables can imply a false order or hierarchy that doesn't exist, which can mislead the model.\n",
        "*   **Pros:** Simple, memory-efficient.\n",
        "*   **Cons:** Implies an arbitrary order or magnitude to nominal data, which can negatively impact model performance.\n",
        "\n",
        "#### 2. One-Hot Encoding\n",
        "\n",
        "*   **Description:** Creates new binary columns (dummy variables) for each category present in the original feature. If a category is present for a given observation, the corresponding new column gets a `1`, and `0` otherwise. For `Color` (Red, Green, Blue), it would create three new columns: `Color_Red`, `Color_Green`, `Color_Blue`.\n",
        "*   **When to Use:** Ideal for **nominal categorical variables** where there is no inherent order. It avoids imposing any artificial order on the data.\n",
        "*   **Pros:** Does not imply order, widely applicable.\n",
        "*   **Cons:** Can significantly increase the dimensionality of the dataset (curse of dimensionality), especially with many categories, which can lead to sparsity and increased training time.\n",
        "\n",
        "#### 3. Ordinal Encoding\n",
        "\n",
        "*   **Description:** Similar to Label Encoding but explicitly used for ordinal variables, where the integer assignment respects the order of the categories. The user explicitly defines the order.\n",
        "*   **When to Use:** When the categorical variable has a clear and meaningful order.\n",
        "*   **Pros:** Preserves the order information, more appropriate than Label Encoding for ordinal data.\n",
        "*   **Cons:** Requires manual mapping of categories to integers.\n",
        "\n",
        "#### 4. Target Encoding (Mean Encoding)\n",
        "\n",
        "*   **Description:** Replaces each category with the mean of the target variable for that category. For example, if a `City` category has an average `Sale` value of $1000, all instances of that `City` will be replaced with $1000.\n",
        "*   **When to Use:** Useful for high cardinality categorical variables (many unique categories) where one-hot encoding would create too many features.\n",
        "*   **Pros:** Reduces dimensionality, captures information about the target variable.\n",
        "*   **Cons:** Prone to overfitting, especially if not regularized. Requires careful cross-validation to avoid data leakage.\n",
        "\n",
        "#### 5. Frequency/Count Encoding\n",
        "\n",
        "*   **Description:** Replaces each category with the count or frequency of its occurrence in the dataset.\n",
        "*   **When to Use:** Can be useful for high cardinality features or when the frequency of a category is believed to be informative.\n",
        "*   **Pros:** Simple, reduces dimensionality.\n",
        "*   **Cons:** Categories with the same frequency will be treated identically, potentially losing information.\n",
        "\n",
        "#### 6. Binary Encoding\n",
        "\n",
        "*   **Description:** First, categories are mapped to integers, then those integers are converted into binary code. Each bit of the binary code creates a new column. For example, `Red` (1) -> `01`, `Green` (2) -> `10`, `Blue` (3) -> `11`.\n",
        "*   **When to Use:** A good compromise between One-Hot Encoding and Label Encoding for high cardinality variables. Reduces dimensionality more than one-hot encoding but retains more information than label encoding for nominal variables.\n",
        "*   **Pros:** Reduces dimensionality compared to one-hot encoding, handles high cardinality.\n",
        "*   **Cons:** Can be less interpretable than one-hot encoding.\n",
        "\n",
        "The choice of encoding method often involves experimentation and depends on the specific dataset, the type of categorical variable, and the machine learning model being employed."
      ]
    }
  ]
}