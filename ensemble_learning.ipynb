{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble Learning | Assignment**"
      ],
      "metadata": {
        "id": "9r4EiOeQ4Iot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.**"
      ],
      "metadata": {
        "id": "IHdE3MiC4WYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble Learning is a machine learning technique that combines multiple individual models (often called 'weak learners' or 'base learners') to achieve better predictive performance than could be obtained from any single model. The key idea behind it is to leverage the 'wisdom of the crowd' principle: by combining the predictions of several diverse models, the ensemble can reduce errors, improve robustness, and increase accuracy. Each individual model might have its own strengths and weaknesses, but when their predictions are aggregated, the biases and variances tend to cancel each other out, leading to a more reliable and accurate overall prediction."
      ],
      "metadata": {
        "id": "8wX-i-ha4jBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: What is the difference between Bagging and Boosting?**"
      ],
      "metadata": {
        "id": "Byi43R7d4ovN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0551e266"
      },
      "source": [
        "Bagging and Boosting are both ensemble learning techniques that combine multiple weak learners to create a strong learner, but they differ significantly in their approach:\n",
        "\n",
        "**Bagging (Bootstrap Aggregating):**\n",
        "*   **Goal:** To reduce variance and prevent overfitting.\n",
        "*   **How it works:**\n",
        "    *   **Parallel:** Base learners are built independently (in parallel).\n",
        "    *   **Bootstrapping:** Multiple subsets of the original training data are created by sampling with replacement (bootstrapping).\n",
        "    *   **Model Training:** A base learner (e.g., decision tree) is trained on each of these subsets.\n",
        "    *   **Aggregation:** For regression, predictions are averaged; for classification, predictions are combined by majority voting.\n",
        "*   **Key Characteristics:**\n",
        "    *   **Reduces Variance:** By averaging or voting, it smooths out the individual model biases and reduces the overall variance of the ensemble.\n",
        "    *   **Less prone to overfitting:** Due to the averaging/voting mechanism and diverse training sets.\n",
        "    *   **Examples:** Random Forest (which is essentially Bagging applied to decision trees).\n",
        "\n",
        "**Boosting:**\n",
        "*   **Goal:** To reduce bias and convert weak learners into strong learners.\n",
        "*   **How it works:**\n",
        "    *   **Sequential:** Base learners are built sequentially, with each new model attempting to correct the errors of the previous ones.\n",
        "    *   **Weighted Data:** Each iteration, the algorithm focuses more on the misclassified or high-error instances from the previous models by assigning them higher weights.\n",
        "    *   **Model Training:** A base learner is trained on the weighted data.\n",
        "    *   **Aggregation:** Predictions are combined using a weighted average, where models that performed better on challenging instances might have higher influence.\n",
        "*   **Key Characteristics:**\n",
        "    *   **Reduces Bias:** By iteratively focusing on difficult examples, it effectively learns from mistakes and improves overall accuracy.\n",
        "    *   **More prone to overfitting:** If not properly regularized, as it can over-specialize on the training data due to its sequential error correction.\n",
        "    *   **Examples:** AdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM, CatBoost.\n",
        "\n",
        "**Summary of Differences:**\n",
        "\n",
        "| Feature         | Bagging (e.g., Random Forest)                                  | Boosting (e.g., AdaBoost, XGBoost)                               |\n",
        "| :-------------- | :------------------------------------------------------------- | :----------------------------------------------------------------- |\n",
        "| **Approach**    | Parallel, independent training                                 | Sequential, dependent training                                     |\n",
        "| **Goal**        | Reduce variance, prevent overfitting                           | Reduce bias, convert weak learners to strong learners              |\n",
        "| **Data Usage**  | Each model trained on a bootstrap sample of the original data  | Each model trained on re-weighted data, focusing on previous errors |\n",
        "| **Error Focus** | Each model treats errors equally                             | Each model focuses on errors made by previous models              |\n",
        "| **Weighting**   | Equal weighting for models (or simple majority vote)           | Models weighted based on their performance, data points weighted   |\n",
        "| **Complexity**  | Simpler to parallelize                                       | More complex, inherently sequential                               |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**"
      ],
      "metadata": {
        "id": "JVU3Lme-5UTV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeadfa20"
      },
      "source": [
        "**Bootstrap Sampling:**\n",
        "\n",
        "Bootstrap sampling (also known as bootstrapping) is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. In simpler terms, it involves creating multiple new datasets (bootstrap samples) from an original dataset by randomly selecting observations with replacement. This means that an observation can be selected multiple times in a single bootstrap sample, and some observations might not be selected at all.\n",
        "\n",
        "Key characteristics of bootstrap sampling:\n",
        "*   **Sampling with Replacement:** Each time an observation is selected from the original dataset to form a bootstrap sample, it is returned to the original dataset, making it available for re-selection.\n",
        "*   **Same Size:** Each bootstrap sample typically has the same size as the original dataset.\n",
        "*   **Diverse Samples:** Because of sampling with replacement, each bootstrap sample will be slightly different from the original dataset and from each other, containing some duplicates and missing some original observations.\n",
        "\n",
        "**Role in Bagging Methods like Random Forest:**\n",
        "\n",
        "Bootstrap sampling is a fundamental component of Bagging (Bootstrap Aggregating) methods, and it plays a crucial role in the success of algorithms like Random Forest. Here's how:\n",
        "\n",
        "1.  **Creating Diverse Training Sets:** In Bagging, instead of training a single model on the entire dataset, multiple base models (e.g., decision trees in Random Forest) are trained on different bootstrap samples of the original training data. This ensures that each base model sees a slightly different version of the data.\n",
        "\n",
        "2.  **Reducing Variance:** Because each base model is trained on a different subset of the data, they will likely make different errors and have different biases. When their predictions are combined (e.g., by averaging for regression or majority voting for classification), these individual errors and biases tend to cancel each other out. This process effectively reduces the variance of the overall ensemble model, making it more robust and less prone to overfitting than any single base model.\n",
        "\n",
        "3.  **Increasing Model Stability:** By training on diverse samples, the ensemble becomes more stable. The overall prediction is less sensitive to small changes in the training data, as the individual models' sensitivities are averaged out.\n",
        "\n",
        "4.  **Enabling Parallelization:** Since each base model is trained independently on its own bootstrap sample, the training process can be easily parallelized, making Bagging methods computationally efficient.\n",
        "\n",
        "In essence, bootstrap sampling is the mechanism that introduces diversity among the base learners in Bagging. This diversity is key to the ensemble's ability to reduce variance and improve overall predictive performance, making it a powerful technique for creating robust machine learning models like the Random Forest."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**"
      ],
      "metadata": {
        "id": "JGVwxFNh5hYR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72c7984c"
      },
      "source": [
        "**Out-of-Bag (OOB) Samples:**\n",
        "\n",
        "Out-of-Bag (OOB) samples refer to the data points from the original training dataset that were *not* included in a particular bootstrap sample used to train a base learner in a Bagging ensemble (like a Random Forest). In bootstrap sampling, approximately one-third of the original data is left out of any given bootstrap sample. These left-out data points for each base learner are its OOB samples.\n",
        "\n",
        "To illustrate:\n",
        "1.  When creating a bootstrap sample for a base model, roughly 63.2% of the original dataset is selected (with replacement).\n",
        "2.  The remaining ~36.8% of the data points that were *not* selected for that specific bootstrap sample constitute the OOB samples for that particular base model.\n",
        "\n",
        "**How OOB Score is Used to Evaluate Ensemble Models:**\n",
        "\n",
        "The OOB score provides a convenient and efficient way to evaluate the performance of Bagging ensemble models (especially Random Forests) without the need for a separate validation set or cross-validation. Here's how it works:\n",
        "\n",
        "1.  **Prediction by Unseen Data:** For each data point in the original training set, only the base models that did *not* use that data point in their training (i.e., those for which that data point was an OOB sample) are used to make a prediction for that data point.\n",
        "\n",
        "2.  **Aggregating OOB Predictions:** For a given data point, predictions are collected from all base models for which it was an OOB sample. These predictions are then aggregated (e.g., averaged for regression, or majority voted for classification) to produce a final OOB prediction for that data point.\n",
        "\n",
        "3.  **Calculating the OOB Score:** Once OOB predictions have been made for all data points in the original training set (or at least for a sufficient portion), the OOB score is calculated by comparing these OOB predictions with the true labels (or values). Common metrics for OOB score include:\n",
        "    *   **Accuracy:** For classification, the proportion of correctly classified OOB samples.\n",
        "    *   **R-squared or Mean Squared Error (MSE):** For regression.\n",
        "\n",
        "**Advantages of OOB Score:**\n",
        "\n",
        "*   **Internal Validation:** It acts as an internal, unbiased estimate of the model's generalization error, similar to using a validation set. This is because each data point is evaluated by models that have not seen it during their training.\n",
        "*   **Efficiency:** It eliminates the need for splitting the data into separate training and validation sets, allowing the model to utilize all available data for training while still providing a robust performance estimate.\n",
        "*   **Computational Savings:** Since the OOB evaluation happens during the training process, it avoids the additional computational cost of performing k-fold cross-validation or training on a reduced dataset for a validation split.\n",
        "\n",
        "In summary, OOB samples are the data points left out of each bootstrap sample, and the OOB score leverages these samples to provide an accurate and efficient estimate of the ensemble model's performance without requiring external validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n"
      ],
      "metadata": {
        "id": "NH1DTpzv5vi3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fddb4c7a"
      },
      "source": [
        "Feature importance analysis is a crucial aspect of understanding machine learning models, helping to identify which features contribute most to the model's predictions. While both Decision Trees and Random Forests can provide feature importances, there are significant differences in how they are calculated and interpreted due to their underlying structures.\n",
        "\n",
        "### **Feature Importance in a Single Decision Tree**\n",
        "\n",
        "**How it's Calculated:**\n",
        "In a single Decision Tree, feature importance is typically determined by the reduction in impurity (e.g., Gini impurity for classification, Mean Squared Error for regression) that a feature brings when it's used to split a node. The more a feature reduces impurity across all splits it's involved in, the higher its importance score.\n",
        "\n",
        "**Characteristics:**\n",
        "*   **Greedy and Local:** The importance scores are very specific to the particular tree structure. A feature might appear very important if it's chosen for a high-level split early in the tree, even if other features might be equally or more important if the tree had been constructed differently.\n",
        "*   **Instability:** Small changes in the training data can lead to drastically different tree structures and, consequently, different feature importance rankings.\n",
        "*   **Bias towards High Cardinality Features:** Features with many unique values or continuous features can sometimes be artificially inflated in importance because they offer more potential split points, which might coincidentally lead to large impurity reductions. This doesn't necessarily mean they are genuinely more predictive.\n",
        "*   **Tree-Specific:** The importance values only reflect the contribution of features within that *specific* decision tree.\n",
        "\n",
        "### **Feature Importance in a Random Forest**\n",
        "\n",
        "**How it's Calculated:**\n",
        "Random Forests, being an ensemble of many Decision Trees, aggregate the feature importances from all individual trees. The most common method is \"Mean Decrease in Impurity\" (MDI) or \"Gini Importance\":\n",
        "1.  For each feature in each individual decision tree, the impurity reduction from that feature is calculated, just like in a single tree.\n",
        "2.  These impurity reductions are then averaged across all the trees in the forest. The final importance score for a feature is the average of its importance across all trees.\n",
        "\n",
        "**Characteristics:**\n",
        "*   **More Robust and Stable:** By averaging over many trees, the feature importances in a Random Forest are much more stable and less prone to the instability caused by small data variations or specific tree structures. The \"randomness\" introduced by bootstrapping and feature subsampling helps to decorrelate the trees and make the averaged importances more reliable.\n",
        "*   **Global View:** Random Forests provide a more global assessment of feature importance. If a feature is consistently important across many different trees (even if its position varies), it will receive a high aggregate score.\n",
        "*   **Reduced Bias (but still present):** While the averaging helps, Random Forests can still exhibit some bias towards high cardinality or continuous features, though generally less pronounced than in a single decision tree due to the ensemble's diversity.\n",
        "*   **Better Generalization:** The aggregated importance scores tend to generalize better to unseen data, as they reflect a more comprehensive understanding of feature contributions across various data subsets and tree configurations.\n",
        "\n",
        "### **Comparison Summary:**\n",
        "\n",
        "| Feature             | Single Decision Tree                                      | Random Forest                                                   |\n",
        "| :------------------ | :-------------------------------------------------------- | :-------------------------------------------------------------- |\n",
        "| **Calculation Basis** | Impurity reduction in a single tree                       | Average impurity reduction across many trees                      |\n",
        "| **Stability**       | Low; sensitive to data variations and tree structure      | High; robust due to averaging over an ensemble                  |\n",
        "| **Bias**            | Prone to bias towards high cardinality/continuous features | Less prone to bias, but can still exist                         |\n",
        "| **Interpretation**  | Local to the specific tree                                | Global, more reliable, and generalizable                        |\n",
        "| **Reliability**     | Lower, can be misleading                                  | Higher, better indicator of true predictive power               |\n",
        "\n",
        "In essence, while a single Decision Tree gives you a snapshot of how features were utilized in one specific model, a Random Forest provides a more generalized, robust, and reliable measure of a feature's overall importance across various model configurations, making it generally preferred for feature importance analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6: Write a Python program to: ● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() ● Train a Random Forest Classifier ● Print the top 5 most important features based on feature importance scores.**\n"
      ],
      "metadata": {
        "id": "sQZGXwyc6Es9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cfa34b0",
        "outputId": "672fbd50-4d32-41d0-d978-e19b1ebaefb7"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "bcs = load_breast_cancer()\n",
        "X = pd.DataFrame(bcs.data, columns=bcs.feature_names)\n",
        "y = bcs.target\n",
        "\n",
        "print(\"Dataset loaded successfully. Shape of features (X):\", X.shape)\n",
        "print(\"Shape of target (y):\", y.shape)\n",
        "\n",
        "# 2. Train a Random Forest Classifier\n",
        "# Using default parameters for simplicity, but in a real scenario, hyperparameter tuning would be beneficial.\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "print(\"\\nRandom Forest Classifier trained successfully.\")\n",
        "\n",
        "# 3. Print the top 5 most important features based on feature importance scores\n",
        "feature_importances = pd.Series(rf_classifier.feature_importances_, index=X.columns)\n",
        "\n",
        "# Sort features by importance in descending order and get the top 5\n",
        "top_5_features = feature_importances.nlargest(5)\n",
        "\n",
        "print(\"\\nTop 5 most important features:\")\n",
        "print(top_5_features)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully. Shape of features (X): (569, 30)\n",
            "Shape of target (y): (569,)\n",
            "\n",
            "Random Forest Classifier trained successfully.\n",
            "\n",
            "Top 5 most important features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Write a Python program to: ● Train a Bagging Classifier using Decision Trees on the Iris dataset ● Evaluate its accuracy and compare with a single Decision Tree**\n"
      ],
      "metadata": {
        "id": "I18SI1HW6hhR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfbeb966",
        "outputId": "2208e87a-7f23-471b-c077-25d9db3fd3a9"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "print(\"Iris dataset loaded successfully. Shape of features (X):\", X.shape)\n",
        "print(\"Shape of target (y):\", y.shape)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "\n",
        "# 2. Train a single Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy for the single Decision Tree\n",
        "y_pred_dt = dt_classifier.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"\\nAccuracy of a single Decision Tree: {accuracy_dt:.4f}\")\n",
        "\n",
        "# 3. Train a Bagging Classifier using Decision Trees\n",
        "# base_estimator is the DecisionTreeClassifier\n",
        "bag_classifier = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42), # Base estimator\n",
        "    n_estimators=100, # Number of base estimators\n",
        "    max_samples=1.0, # Use all samples for each base estimator (bootstrapping handles sampling)\n",
        "    max_features=1.0, # Use all features for each base estimator\n",
        "    bootstrap=True, # Sample with replacement\n",
        "    random_state=42\n",
        ")\n",
        "bag_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy for the Bagging Classifier\n",
        "y_pred_bag = bag_classifier.predict(X_test)\n",
        "accuracy_bag = accuracy_score(y_test, y_pred_bag)\n",
        "print(f\"Accuracy of Bagging Classifier (100 Decision Trees): {accuracy_bag:.4f}\")\n",
        "\n",
        "# 4. Compare accuracies\n",
        "print(\"\\n--- Comparison ---\")\n",
        "if accuracy_bag > accuracy_dt:\n",
        "    print(\"The Bagging Classifier performed better than the single Decision Tree.\")\n",
        "elif accuracy_bag < accuracy_dt:\n",
        "    print(\"The single Decision Tree performed better than the Bagging Classifier.\")\n",
        "else:\n",
        "    print(\"Both models achieved the same accuracy.\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris dataset loaded successfully. Shape of features (X): (150, 4)\n",
            "Shape of target (y): (150,)\n",
            "\n",
            "Training set size: 105 samples\n",
            "Testing set size: 45 samples\n",
            "\n",
            "Accuracy of a single Decision Tree: 0.9333\n",
            "Accuracy of Bagging Classifier (100 Decision Trees): 0.9333\n",
            "\n",
            "--- Comparison ---\n",
            "Both models achieved the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8: Write a Python program to: ● Train a Random Forest Classifier ● Tune hyperparameters max_depth and n_estimators using GridSearchCV ● Print the best parameters and final accuracy.**"
      ],
      "metadata": {
        "id": "OIrhF3Sy7Azy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33f9f115",
        "outputId": "eb016cc6-0745-4d5c-b676-c8793d3e9af0"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset (reusing from previous question, but good to include for standalone execution)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "print(\"Iris dataset loaded successfully. Shape of features (X):\", X.shape)\n",
        "print(\"Shape of target (y):\", y.shape)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "\n",
        "# 2. Define the Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20],     # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5],     # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2]       # Minimum number of samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "# 3. Tune hyperparameters using GridSearchCV\n",
        "print(\"\\nStarting GridSearchCV for hyperparameter tuning...\")\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"GridSearchCV completed.\")\n",
        "\n",
        "# 4. Print the best parameters and best score\n",
        "print(\"\\nBest parameters found:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the final accuracy on the test set\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Final accuracy on the test set with best parameters: {final_accuracy:.4f}\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris dataset loaded successfully. Shape of features (X): (150, 4)\n",
            "Shape of target (y): (150,)\n",
            "\n",
            "Training set size: 105 samples\n",
            "Testing set size: 45 samples\n",
            "\n",
            "Starting GridSearchCV for hyperparameter tuning...\n",
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "GridSearchCV completed.\n",
            "\n",
            "Best parameters found:\n",
            "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 150}\n",
            "Best cross-validation accuracy: 0.9619\n",
            "Final accuracy on the test set with best parameters: 0.9111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: Write a Python program to: ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset ● Compare their Mean Squared Errors (MSE).**"
      ],
      "metadata": {
        "id": "D0AD9bx37pF5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13338298",
        "outputId": "f0126e93-dcef-49e0-8bbf-0e015eacede2"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor # BaggingRegressor uses this as default base_estimator\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X = california_housing.data\n",
        "y = california_housing.target\n",
        "\n",
        "print(\"California Housing dataset loaded successfully. Shape of features (X):\", X.shape)\n",
        "print(\"Shape of target (y):\", y.shape)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"\\nTraining set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "\n",
        "# 2. Train a Bagging Regressor\n",
        "# Using DecisionTreeRegressor as the base estimator, which is the default\n",
        "bag_regressor = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(random_state=42),\n",
        "    n_estimators=100, # Number of base estimators\n",
        "    random_state=42,\n",
        "    n_jobs=-1 # Use all available cores\n",
        ")\n",
        "bag_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate MSE for Bagging Regressor\n",
        "y_pred_bag = bag_regressor.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "print(f\"\\nBagging Regressor trained successfully. MSE: {mse_bag:.4f}\")\n",
        "\n",
        "# 3. Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(\n",
        "    n_estimators=100, # Number of trees in the forest\n",
        "    random_state=42,\n",
        "    n_jobs=-1 # Use all available cores\n",
        ")\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate MSE for Random Forest Regressor\n",
        "y_pred_rf = rf_regressor.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Regressor trained successfully. MSE: {mse_rf:.4f}\")\n",
        "\n",
        "# 4. Compare their Mean Squared Errors (MSE)\n",
        "print(\"\\n--- Comparison of MSE ---\")\n",
        "print(f\"Bagging Regressor MSE:    {mse_bag:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n",
        "\n",
        "if mse_rf < mse_bag:\n",
        "    print(\"The Random Forest Regressor achieved a lower (better) MSE.\")\n",
        "elif mse_bag < mse_rf:\n",
        "    print(\"The Bagging Regressor achieved a lower (better) MSE.\")\n",
        "else:\n",
        "    print(\"Both regressors achieved the same MSE.\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "California Housing dataset loaded successfully. Shape of features (X): (20640, 8)\n",
            "Shape of target (y): (20640,)\n",
            "\n",
            "Training set size: 16512 samples\n",
            "Testing set size: 4128 samples\n",
            "\n",
            "Bagging Regressor trained successfully. MSE: 0.2559\n",
            "Random Forest Regressor trained successfully. MSE: 0.2554\n",
            "\n",
            "--- Comparison of MSE ---\n",
            "Bagging Regressor MSE:    0.2559\n",
            "Random Forest Regressor MSE: 0.2554\n",
            "The Random Forest Regressor achieved a lower (better) MSE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to: ● Choose between Bagging or Boosting ● Handle overfitting ● Select base models ● Evaluate performance using cross-validation ● Justify how ensemble learning improves decision-making in this real-world context.**"
      ],
      "metadata": {
        "id": "7Sz5Ciit80gO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Step-by-Step Approach to Using Ensemble Techniques for Loan Default Prediction**\n",
        "\n",
        "### **1. Choosing Between Bagging and Boosting**\n",
        "\n",
        "* **Bagging (e.g., Random Forest)** is preferred when the base model has **high variance** and you want to improve stability and reduce overfitting by averaging many independent models.\n",
        "* **Boosting (e.g., XGBoost, LightGBM)** is preferred when the model suffers from **high bias**, and you want to sequentially reduce errors by giving more weight to misclassified samples.\n",
        "* **In loan-default prediction**, boosting generally performs better because it captures complex non-linear relationships in customer and transaction data.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Handling Overfitting**\n",
        "\n",
        "* Use **cross-validation** to avoid overly optimistic estimates.\n",
        "* Apply **regularization techniques** (L1/L2 penalties, learning-rate reduction, shrinkage).\n",
        "* Limit model complexity (tree depth, number of estimators, minimum samples per leaf).\n",
        "* Use **early stopping** based on validation performance.\n",
        "* Use **subsampling** (row/column sampling in boosting) to increase generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Selecting Base Models**\n",
        "\n",
        "* Choose **diverse and complementary models**:\n",
        "\n",
        "  * **Decision trees** (high-variance, good for bagging).\n",
        "  * **Shallow trees / weak learners** (for boosting).\n",
        "  * **Logistic Regression** for interpretability and calibration.\n",
        "* In practice, **Random Forest** for bagging and **Gradient Boosted Trees** for boosting are most effective on structured financial data.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Evaluating Performance Using Cross-Validation**\n",
        "\n",
        "* Use **Stratified k-Fold Cross-Validation** to maintain the default/non-default class ratio in each fold.\n",
        "* Tune hyperparameters inside the CV loop to avoid leakage.\n",
        "* Evaluate using metrics relevant to imbalanced data:\n",
        "\n",
        "  * **AUC-ROC**, **Precision-Recall AUC**, **F1**, or **Top-N% Precision**.\n",
        "* If the data is time-dependent, use **time-series CV** to avoid using future data for training.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Justification: Why Ensemble Learning Improves Decision-Making**\n",
        "\n",
        "* **Higher predictive accuracy** leads to better identification of high-risk borrowers.\n",
        "* **Reduced variance** (bagging) gives more stable and reliable predictions across different samples.\n",
        "* **Reduced bias** (boosting) captures subtle patterns in transaction behaviour.\n",
        "* **Better calibration** provides more accurate risk scores for credit decisions and pricing.\n",
        "* **Lower financial risk**: improved classification reduces default losses and supports smarter loan approval thresholds.\n",
        "\n"
      ],
      "metadata": {
        "id": "HeejK8IxCI0e"
      }
    }
  ]
}